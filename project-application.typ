#import "project-application-template.typ": *

// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "面向无人机的3D目标检测算法",
  name: "赖宇",
  idnum: "202102001020",
  major2: "无", 
  major1: "人工智能与大数据", 
  college: "计算机学院",
  grade: "2021级", 
  advisor: "邓明堂",
  jobtitle: "副研究员",
  unit: "国防科大计算机学院学员一大队学员五队", 
  comments: "较好明确了毕业设计的计划和目标，研究项目具有较好的现实意义，制定了可行的实施方案，正确评估了研究过程中可能遇到的问题和相应的解决方案，文献调研充分，同意开题。"
)

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!

= 面向无人机的3D目标检测算法

无人机（Unmanned Aerial Vehicles）是指无人驾驶的飞行机器。近年来，由于其卓越的机动性，无人机被广泛部署在交通监控、精准农业、灾害管理和野生动物监控等领域。与传统路侧端监的控摄像头相比，无人机提供更高的效率和适应性，对推进计算机视觉（Computer Vision）的众多应用至关重要。在这些应用中，鲁棒的对象检测和跟踪对于无人机的有效部署至关重要。然而，现有的无人机应用模型主要是针对传统的2D感知任务设计的，例如LAM-YOLO@LAM-YOLO24 和Drone-TOOD@Drone-TOOD24 ，这限制了需要对环境进行3D理解的实际应用的发展。在基于视觉的机器人系统中，3D感知扮演着重要角色，使其能够处理2D感知无法胜任的复杂任务。虽然对于无人机来说，3D视觉仍然是相对较新的技术，但它提供了在3D环境中捕获对象的完整维度数据的能力。

随着自动驾驶技术的发展，研究人员开始关注3D视觉下多角度的目标检测技术，出现了DERE3D@detr3d21、PETR@PETR22、PETRv2@PETRv2、BEVFormer@BEVFormer22 等车载3D多视角目标检测模型。然而，目前尚未出现专注于无人机的3D多视角目标检测模型。同时，针对无人机3D多视角目标检测任务的数据集UAV3D日前发布，经过测试，先前的3D多视角目标检测模型在该数据集上表现均不理想，可能的原因在于车载模型的泛化性问题，这说明当前的研究结果并不适用于无人机领域。因此，有必要研究无人机的3D多视角目标检测模型，以提升无人机对于3D环境的理解能力。

= 国内外研究现状及发展趋势

本项目主要研究面向无人机的3D目标检测算法。我们将介绍无人机、目标检测算法、3D目标检测等几个方面的相关工作。

== 无人机与深度学习

无人机因价格便宜、使用方便、对人员安全以及操作人员培训简单而越来越受欢迎。@onmonitoring18 这些优势，加上其的分辨率和强大的跟踪特性，促使它们在各种环境中的使用越来越多。无人机已被用于环境监测，包括空气污染、地表温度、洪水危险、森林火灾、道路表面损坏、地形监测、行人交通监测和灾害疏散。@dronesreview19 例如，许多人因为可以通过移动设备控制的先进产品而提高了生活水平。汽车技术通过提供关于交通的最新和精确信息来帮助驾驶员。无人机有多种规格、尺寸和配置。它们被归类为四大类别：固定翼、混合固定翼、单旋翼和多旋翼，同时考虑旋翼的数量。固定翼无人机适合于航空测量和绘图，因为它们稳定且续航时间长。混合固定翼无人机结合了自动化和手动滑翔，提供了可操作性和效率之间的平衡。单旋翼无人机虽然更复杂且成本更高，但为特定任务（如详细的地形测量）提供了卓越的精确度。最后，多旋翼无人机，尤其是四旋翼无人机，因其敏捷性、垂直起降能力和常用于监控和航空摄影应用而受到高度重视。多旋翼无人机可以是三旋翼、四旋翼、六旋翼或八旋翼。@aidrones24


得益于各种技术和方法的发展，人工智能（Artificial Intelligence）近几十年来经历了显著的进步。机器学习（Machine Learning）技术因其能够通过与环境的互动提供决策自主性而脱颖而出。机器学习是不断发展的数据科学领域的重要组成部分。通过使用统计方法，模型被训练以在各种基于数据挖掘的项目中进行分类或预测，从而发现关键信息。在这一演变的背景下，深度学习（Deep Learning）作为机器学习中的先进技术而出现，它由具有多层的人工神经网络组成。这些深层结构使我们能够处理复杂任务，如识别图像中的模式或理解自然语言。存在多种ML算法，根据一些基本标准对它们进行预先分类是有益的。其中一个最重要和最显著的标准与算法的训练方式有关。在这种分类中，区分了四种主要方法：监督学习、无监督学习以及强化学习。@aidrones24

计算机硬件和软件的最新发展使得人工智能成为几乎所有与工程相关的研究领域的关键组成部分。人工智能是解决那些没有明确答案或传统方法需要大量人为干预的难题的强大工具。人工智能与传统认知算法之间的一个显著区别在于，人工智能可以自动提取特征。这取代了昂贵的手工特征工程。无人机的许多问题，如高功率/能源消耗和实时需求，也是边缘计算和边缘人工智能的优点，如低能耗和低延迟。深度学习和无人机（Unmaned Aerial Vehicles）有潜力彻底改变交通部门的交通监控。@indepthaidrones24


// == 目标检测算法
// // 这里用的 Object Detection in 20 Years: A Survey

// 目标检测是一项重要的计算机视觉任务，用于检测数字图像中某一类（如人类、动物或汽车）的视觉物体实例。物体检测的目标是开发计算模型和技术，以提供计算机视觉应用所需的最基本知识之一：什么物体在哪里？物体检测的两个最重要指标是准确度（包括分类准确度和定位准确度）和速度。我们主要介绍基于深度学习的检测时期（2014年之后）的目标检测算法。下面我们从基于CNN的两阶段检测器和基于CNN的单阶段检测器两方面介绍目标检测算法。

// === 基于CNN的两阶段检测器。

// 2014年，R. Girshick等人率先打破僵局，提出了带有CNN特征的区域（RCNN），将卷积神经网络网络（CNN）引入物体检测领域，RCNN也是首个两阶段检测器。

// RCNN：RCNN 背后的理念很简单：首先通过选择性搜索提取一组对象建议（对象候选框）。然后将每个建议重新缩放为固定大小的图像，并输入到在 ImageNet 上预训练的 CNN 模型（例如 AlexNet ）中以提取特征。最后，使用线性 SVM 分类器预测每个区域内是否存在对象并识别对象类别。 RCNN 在 VOC07 上的性能显著提升，平均精度 (mAP) 从 33.7%（DPM-v5 ）大幅提升至 58.5%。虽然 RCNN 取得了很大进展，但其缺点也很明显：对大量重叠建议（来自一张图像的 2000 多个框）进行冗余特征计算导致检测速度极慢（使用 GPU 每张图像 14 秒）。同年晚些时候，SPPNet 被提出并解决了这个问题。

// SPPNet：2014 年，K. He 等人提出了空间金字塔池化网络 (SPPNet) 。以前的 CNN 模型需要固定大小的输入，例如 AlexNet 的 224x224 图像 。SPPNet 的主要贡献是引入了空间金字塔池化 (SPP) 层，这使得 CNN 能够生成固定长度的表示，而不管图像/感兴趣区域的大小如何，而无需重新缩放它。 当使用 SPPNet 进行对象检测时，只需从整个图像计算一次特征图，然后就可以生成任意区域的固定长度表示来训练检测器，从而避免重复计算卷积特征。SPPNet 比 R-CNN 快 20 倍以上，而不会牺牲任何检测精度（VOC07 mAP=59.2%）。虽然 SPPNet 有效地提高了检测速度，但它仍然存在一些缺点：首先，训练仍然是多阶段的；其次，SPPNet 只对其全连接层进行微调，而简单地忽略了所有前面的层。后来在次年，Fast RCNN 被提出并解决了这些问题。

// Fast RCNN：2015 年，R. Girshick 提出了 Fast RCNN 检测器 ，这是 R-CNN 和 SPPNet 的进一步改进。Fast RCNN 使我们能够在相同的网络配置下同时训练检测器和边界框回归器。在 VOC07 数据集上，Fast RCNN 将 mAP 从 58.5%（RCNN）提高到 70.0%，同时检测速度比 R-CNN 快 200 倍以上。虽然 Fast-RCNN 成功地整合了 R-CNN 和 SPPNet 的优势，但其检测速度仍然受到提案检测的限制（有关更多详细信息，请参阅第 II-C1 节）。那么，一个问题自然而然地出现了：“我们能用 CNN 模型生成对象提案吗？”后来，Faster R-CNN 回答了这个问题。

// Faster RCNN：2015 年，S. Ren 等人在 Fast RCNN 之后不久提出了 Faster RCNN 检测器 。Faster RCNN 是第一个近实时的深度学习检测器（COCO mAP\@.5=42.7%，VOC07 mAP=73.2%，使用 ZF-Net 时为 17fps ）。Faster-RCNN 的主要贡献是引入了区域提议网络 (RPN)，从而实现了几乎无成本的区域提议。从 R-CNN 到 Faster RCNN，对象检测系统的大多数单个模块，例如提议检测、特征提取、边界框回归等，已逐渐集成到统一的端到端学习框架中。虽然 Faster RCNN 突破了 Fast RCNN 的速度瓶颈，但在后续的检测阶段仍然存在计算冗余。后来，人们提出了各种改进，包括 RFCN 和 Light head RCNN 。 

// 特征金字塔网络 (FPN)：2017 年，T.-Y. Lin 等人提出了 FPN 。在 FPN 之前，大多数基于深度学习的检测器仅在网络顶层的特征图上运行检测。虽然 CNN 较深层中的特征有利于类别识别，但不利于定位物体。为此，FPN 中开发了一种具有横向连接的自上而下的架构，用于在所有尺度上构建高级语义。由于 CNN 通过其前向传播自然形成特征金字塔，FPN 在检测各种尺度的物体方面显示出巨大的进步。在基本的 Faster R-CNN 系统中使用 FPN，它在 COCO 数据集上实现了最先进的单模型检测结果，没有任何花哨的花哨功能（COCO mAP\@.5=59.1%）。FPN 现在已成为许多最新检测器的基本构建块。


// === 基于CNN的单阶段检测器。

// 大多数双阶段检测器遵循由粗到精的处理范式。粗阶段致力于提高召回率，而精阶段在粗检测的基础上细化定位，并更注重判别能力。它们可以轻松获得高精度，而无需任何花哨的修饰，但由于速度慢、复杂度高，在工程上很少使用。相反，单阶段检测器可以在一步推理中检索所有物体。 它们以实时性和易于部署的特点受到移动设备的青睐，但在检测密集和小物体时，它们的性能会明显下降。

// YOLO（You Only Look Once）：YOLO 由 R. Joseph 等人于 2015 年提出，是深度学习时代第一个单阶段检测器 。YOLO 速度极快：快速版 YOLO 运行速度为 155fps，VOC07 mAP=52.7%，增强版运行速度为 45fps，VOC07 mAP=63.4%。YOLO 采用了与两阶段检测器完全不同的思路：将单个神经网络应用于整幅图像，该网络将图像划分为区域，并同时预测每个区域的边界框和概率。尽管 YOLO 的检测速度有了很大的提升，但与两阶段检测器相比，其定位精度有所下降，尤其是对于一些小物体。YOLO 的后续版本 以及后来提出的 SSD 更加关注这一问题。最近，YOLOv7 被提出，这是 YOLOv4 团队的后续工作。通过引入动态标签分配和模型结构重参数化等优化结构，它在速度和准确率方面优于大多数现有的物体检测器（范围从 5 FPS 到 160 FPS）。

// 单次多框检测器（SSD）：SSD 由 W. Liu 等人于 2015 年提出。SSD 的主要贡献是引入了多参考和多分辨率检测技术（将在 II-C1 节中介绍），这显著提高了单阶段检测器的检测精度，尤其是对于一些小物体。SSD 在检测速度和精度方面都具有优势（COCO mAP\@.5=46.5%，快速版本以 59fps 的速度运行）。SSD 与之前的检测器之间的主要区别在于，SSD 在网络的不同层上检测不同尺度的物体，而之前的检测器仅在其顶层运行检测。

// RetinaNet：尽管单阶段检测器速度快、简单易用，但多年来其准确率一直落后于双阶段检测器。T.-Y. Lin 等人探究了背后的原因，并于 2017 年提出了 RetinaNet 。他们发现，在密集检测器训练过程中遇到的极端前景-背景类别不平衡是主要原因。 为此，RetinaNet 引入了一种名为“focal loss”的新损失函数，通过重塑标准交叉熵损失，使检测器在训练期间将更多注意力放在困难的、错误分类的示例上。Focal Loss 使单阶段检测器能够实现与双阶段检测器相当的准确率，同时保持非常高的检测速度（COCO mAP\@.5=59.1%）。

// CornerNet：以前的方法主要使用锚框来提供分类和回归参考。物体在数量、位置、尺度、比例等方面经常发生变化。为了实现高性能，它们必须遵循设置大量参考框以更好地匹配 GT 的路径。然而，网络将遭受进一步的类别不平衡、大量手工设计的超参数和较长的收敛时间。为了解决这些问题，H. Law 等人 放弃了以前的检测范式，将任务视为关键点（框的角）预测问题。获得关键点后，它将使用额外的嵌入信息对角点进行解耦和重新分组以形成边界框。 CornerNet 优于当时大多数单阶段检测器（COCO mAP\@.5=57.8%）。

// CenterNet：X. Zhou 等人于 2019 年提出了 CenterNet 。 它也遵循基于关键点的检测范式，但消除了昂贵的后处理，例如基于组的关键点分配（在 CornerNet 、ExtremeNet 等中）和 NMS，从而产生了一个完全端到端的检测网络。CenterNet 将对象视为单个点（对象的中心），并基于参考中心点回归其所有属性（例如大小、方向、位置、姿势等）。该模型简单而优雅，它可以将 3-D 对象检测、人体姿势估计、光流学习、深度估计和其他任务集成到一个框架中。尽管使用了如此简洁的检测概念，CenterNet 也可以实现相当的检测结果（COCO mAP\@.5=61.1%）。

// DETR：近年来，Transformers 深刻影响了整个深度学习领域，特别是计算机视觉领域。Transformers 摒弃了传统的卷积算子，转而采用仅靠注意力机制的计算，从而克服了 CNN 的局限性，获得了全局尺度的感受野。2020 年，N. Carion 等人提出了 DETR ，他们将物体检测视为一个集合预测问题，并提出了基于 Transformers 的端到端检测网络。至此，物体检测进入了一个无需使用锚框或锚点即可检测物体的新时代。随后，X. Zhu 等人提出了可变形 DETR ，以解决 DETR 收敛时间长、在小物体检测上性能有限的问题，在 MSCOCO 数据集上取得了最佳性能（COCO mAP\@.5=71.9%）。

== 基于RGB图像的3D目标检测算法
// 这里用的文章是 3D Object Detection for Autonomous Driving: A Survey

2D目标检测一定程度上促进了3D目标检测的发展。3D目标检测方法可以分为单视角和多视角两个方面。
@odsurvey22

=== 基于单视角的3D目标检测方法

这些3D目标检测算法在思想上与2D目标检测算法最为接近，仅以单目/立体图像作为输入来预测3D目标实例。@odsurvey22 一般分为三种：基于模板匹配的方法、基于几何属性的方法和基于伪激光雷达的方法。

==== 基于模板匹配的方法

这种方法的主要思想是通过穷尽采样执行2D/3D匹配并将3D候选区与作为代表模板进行评分。典型例子就是早期由 Chen提出的3DOP@3dop15，其对输入的立体图像估计深度，并将图像平面上的像素级坐标投影到三维空间来估计点云。3DOP将候选区生成的问题定义为Markov随机场（MRF）的能量最小化问题，该问题需要对势函数进行精心设计。获取3D目标的候选框后，3DOP使用FastR-CNN@fastrcnn15 方案来回归预测目标位置。
在可能的情况下，汽车仅配备一个摄像头，Chen随后提出了Mono3D@mono3d16 来通过单目相机达到PAR性能。与3DOP不同的是，Mono3D不计算深度信息，而是滑动窗口从3D空间采样3D候选区。当对对象所在地平面与图像平面正交时，可以减少搜索的工作量。3DOP或Mono3D输出制定类别的候选区，这需要为每个类别单独设计方案。然而过度依赖工程和专业领域知识使得模型在复杂场景下的通用性比较差。

==== 基于几何特性的方法

这种方法并不需要大量的候选区来提高召回率，而是直接从精确的2D边界框开始，直接从经验观察中获得几何特性来粗略的估计3D姿态。

Mousavian等人提出的Deeo3DBox@3dbox17 利用几何特性，让3D角的透视投影至少紧密贴在2D边界框的一侧。Li等人提出了GS3D @gsd3d19 ，仅使用单目RGB图像来检测完整的3D实例，而并没有引入额外的数据。GS3D基于Faster RCNN@fasterrcnn17 的框架，额外添加了一个名为2D+O的子网络，用于预测2D边界框和观测的方向。接着GS3D获得粗略的3D框，称为引导。最后，在被送3D子网络进一步细化之前，从2D框和3D框上选择3个可见面进行融合来解决表征模糊的问题。尽管GS3D有了显著的性能提升， 超过了现有的基于单目图像的方法，但其依赖于经验知识，而并不能保证是准确的，且很容易受到对象范围和大小的影响。

==== 基于伪激光雷达的方法

这些方法首先进行深度估计，然后求助于现有的基于点云的方法。

Xu等人提出了MF3D @mf3d18,该算法对图像特征和伪激光雷达进行多层次的融合。具体来说，MF3D首先通过独立的单目深度估计模块计算视差，以获得伪激光雷达。同时，采用标准的 2D 候选区域生成网络，将 RGB 图像与由视差图获得的转换前视图特征融合作为输入。随着2D候选区域的获得，RGB图像和伪激光雷达的特征通过串联融合，以进一步细化。最近，Weng提出了Mono3D-PLiDAR@mono3d-plidar19,通过单目深度估计将输入图像升级成3D相机坐标，即伪激光雷达点(例如,DORN@DORN18)。然后是3D目标检测模型Frustum PointNets@frustumpointnets18 应用于伪激光雷达。Weng等人揭示了伪激光雷达由于单目深度估计的误差，会产生大量的噪声。 体现在两个方面：激光雷达点的局部不对准和深度伪影的问题。为了克服前者，Mono3D-PLiDAR使用2D-3D边界框一致性损失（BBCL)来有监督的训练。为了缓解后者问题，Mono3D-PLiDAR采用Mask R-CNN@MaskR-CNN20 预测的实例掩码代替2D边界框来减少截锥内不相关的点。


=== 基于多视角的3D目标检测方法。
// 这里基于https://zhuanlan.zhihu.com/p/686868635

这些方法首先将多幅图像转换成前视图或鸟瞰图(BEV)展示，在网格中密集以利用 CNN 和标准 2D 检测方案。

Wang等人提出了DETR3D@detr3d21。该算法是DETR在三维目标检测领域的延伸，它通过几何反投影和相机变换矩阵将二维特征提取与三维目标预测联系起来，实现了无需密集深度估计的三维目标检测。DETR3D将多视图检测问题转化为集合到集合的预测任务，通过预设的object queries和神经网络解码出三维空间中的参考点，再将这些点反投影到二维特征图上，通过双线性插值采样特征值，最终通过多头注意力机制和Transformer@transformer17 解码器来优化queries并预测边界框和类别。然而DETR3D存在预测参考点不准确、无法从全局角度进行表示学习等缺点，为此，Liu和Wang等人提出了PETR@PETR22。PETR通过引入3D坐标生成器和3D位置编码器，将三维坐标的位置信息编码为图像特征，从而实现多视图三维目标检测。PETR首先在三维空间中初始化一组均匀分布的anchor points，然后通过MLP网络生成初始对象查询。与DETR3D不同，PETR先预设三维坐标再编码到query，这样做避免了在图像平面找不到对应点的问题，并实现了在三维特征空间中的训练。之后Liu等人提出了PETRv2@PETRv2，PETRv2在PETR的基础上增加了时间建模，通过将前一帧的特征与当前帧的特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。PETRv2通过特征引导的位置编码器将图像特征和三维位置信息结合，隐式引入了视觉先验，提高了模型的性能。

Huang等人提出的BEVDet@BEVDet22 是另一种高性能的多相机三维目标检测方法，它在鸟瞰图（BEV）空间中进行目标检测。BEVDet面临过拟合问题，因为它在BEV空间下过度拟合。为了解决这个问题，BEVDet应用了定制的数据增强策略和尺度NMS（Scale-NMS），以提高模型的泛化能力和检测性能。
之后Huang等人提出了BEVDet4D@BEVDet4D22，BEVDet4D通过将前一帧的特征与当前帧中的相应特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。Li等人提出的BEVFormer@BEVFormer22 利用可变形注意力机制设计了空间交叉注意力和时间自注意力，分别从跨摄像机视图的感兴趣区域提取空间特征和循环融合历史BEV信息，从而实现对三维场景的理解和目标检测。



\

= 项目需要解决的关键理论问题和实际问题

== 如何设计适用无人机3D目标检测领域的模型

由于目前尚无专门针对无人机3D目标检测领域的模型，本项目在前期主要参考车载3D目标检测模型DETR3D@detr3d21，设计端到端的目标检测模型。模型先使用CNN对输入图像提取特征，并使用Transformer@transformer17 的编码器（Encoder）进行处理，并使用可以学习的特定查询序列（Query Tokens）作为解码器（Decoder）的输入，将解码的结果使用两个全连接网络（Full Connected Network）进行处理得到最终的结果。这一端到端框架的优点是减少了非极大值抑制（NMS）等后期处理，极大地提升了运算速度。



== 如何针对无人机目标检测领域的特点来细化改进模型以提升模型精度

相较于车载3D目标检测，无人机3D目标检测的UAV3D@UAV3D24 数据集存在视角移动频繁、目标密度大、目标模糊、目标小等特点。针对该特点，可以设计专门的检测头或引入上下文信息（如目标周围的背景信息），提升小目标的检测精度。还可以在CNN部分使用FPN（多尺度特征金字塔）以捕捉不同尺度的目标特征。除此外，通过替换损失函数也能引导模型更加关注小目标，提升对小目标的检测精度。


== 如何在提升模型精度的同时降低计算复杂度

无人机平台的算力有限，在保证精度的同时降低模型的计算复杂度有着重要的研究价值。StreamPETR研究发现，现有模型如BEV时序模型使用BEV特征导致其对于运动物体检测精度较低，而DETR类型的模型需要与多帧图像进行计算来获取时序相关信息，导致计算量翻倍。设计特征来高效地提取特征并避免多帧图像运算是提升模型精度并降低计算复杂度的关键。


\

= 项目研究的基本方法、实验方案及技术路线的可行性分析

== 项目研究的基本方法和实验方案

=== 研究方法：

==== 文献阅读：阅读3D目标检测的相关文献，掌握3D目标检测的基本原理和常用方法；阅读无人机感知的相关文献，掌握基于无人机目标检测的最新方法和最新进展。

==== 代码实现：实验复现现有的3D目标检测模型。在此基础上，设计适用UAV3D数据集的无人机3D目标检测模型，确定并实现本项目要使用的模型与方法。

==== 结果分析：采用实验法和定量分析法对本项目提出的模型方法进行验证，采用具体的量化指标对模型进行评估。

==== 论文撰写：将本项目得到的结果进行归纳总结，并对研究问题、采用的模型和结果分析进行总结记录。

=== 实施方案：

本项目主要分为两个部分研究面向无人机的3D目标检测算法。第一部分主要是基于UAV3D数据集@UAV3D24 的进行3D目标检测模型的初步设计，并训练验证不同模型架构的可行性；第二部分是基于第一部分的最优模型架构进行进一步优化创新，并将模型尝试应用于无人机目标定位等领域。

==== 基于UAD3D数据集的模型训练

UAV3D数据集是专为无人机 (UAV) 平台的 3D 感知任务而设计@UAV3D24，包含 1,000 个场景（700个训练场景、150个验证场景和150个测试场景）、50万张RGB图像和330万个3D框。本项目基于UAV3D数据集训练设计的模型架构，并使用mAP、NDS、mATE、mASE、mAOE等指标测试，选择综合性能最佳的模型架构。

#{
set text(size: 10.5pt)
figure(image("images/UAV3D_town10.jpg"), caption: "UAD3D示意图")
}
\

==== 模型改进与应用

基于第一部分的模型架构的缺点进行针对性修改，使用消融实验等方法逐步验证每个改进模块的效果，确定其对模型性能的贡献。评估模块是否可以独立发挥作用，或者需要与其他模块协同工作。通过实验找到最优的模块组合，简化模型结构并提升性能。最后尝试将模型应用于无人机目标定位等领域。



\
== 技术路线的可行性分析

项目开展初期对研究现状进行了调研，总结了当前的研究的难点。同时此前也积累了有关目标检测的理论知识与编程经验。已完成了大量相关文献的阅读工作，在实验过程中遇到的相关问题也有一定的解决基础。
总的来说，本项目总体研究方向清晰、研究前景广阔，且具有足够的理论、技术和工具积累，因此是可行的。

\

= 研究条件

== 开展研究应具备的条件

软件条件： Python、Pytorch

硬件条件： 服务器、显卡计算资源

知识储备： 熟悉3D目标识别的研究现状，熟悉Python编程，Pytorch深度学习框架，熟悉Linux操作系统

== 开展研究已经具备的条件

已具备软硬件条件和相应知识储备。

== 可能遇到的困难

=== 3D多视角检测模型的设计以及实现

=== 算法需要多次迭代修改才能有效

== 解决措施 

及时查阅文献，广泛搜集资料，积极与导师，同学交流。


\
= 工作计划

#{
set text(size: 10.5pt)
table(
  columns: (auto, auto, auto),
  inset: 10pt,
  align: horizon,
  table.header(
    [起讫日期], [主要完成研究内容], [预期成果],
  ),
  [2024年11月-2025年1月] , [阅读相关论文文献，准备开题], [了解相关背景，确定项目方向],
  [2025年1月-2025年3月], [分析数据特点，构建模型的输入方式], [完成基础的模型构建],
  [2025年3月-2025年4月], [完成基准实验，并设计实验方案] ,[得到基准实验数据，完成实验方案设计方案],
  [2025年4月-2025年5月], [完成面向无人机3D目标检测的实验和结果分析], [完成最终模型构建并得到最终实验数据],
  [2025年5月-2025年7月], [整理研究成果，撰写毕业论文，准备答辩],[完成毕业论文]
)

}
\
= 器材设备清单

Intel(R) Xeon(R) Gold 6226R CPU \@ 2.90GHz 16核 \* 2 

NVIDA RTX 3090 \* 8

\
= 参考文献
 
#{
set text(size: 10.5pt)

bibliography("references.bib", style: "gb-7714-2015-numeric", title:none)

}