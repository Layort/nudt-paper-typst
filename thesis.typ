#import "@preview/cuti:0.2.1": show-cn-fakebold
#show: show-cn-fakebold
#let tnr = "Times New Roman"
#let fsong = (tnr, "FangSong")
#let song = (tnr, "SimSun")
#let hei = (tnr, "SimHei")
#import "thesis-template.typ": *
#import "templates/i-figured.typ"
#set heading(numbering: "1.")
#show heading: i-figured.reset-counters.with(extra-kinds: ("atom",))
#show figure: i-figured.show-figure.with(extra-prefixes: (atom: "atom:"))
#show math.equation: i-figured.show-equation

#show figure: it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show math.equation: set text(font: "TeX Gyre Termes Math")

#show math.equation.where(block: true): it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show figure.where(kind: table): it => [
  #set figure.caption(position: top)
  #it
]
// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "面向无人机的3D目标检测算法",
  name: "赖宇",
  idnum: "202102001020",
  major2: "无", 
  major1: "人工智能与大数据", 
  college: "计算机学院",
  grade: "2021级", 
  advisor: "邓明堂",
  jobtitle: "副研究员",
  unit: "国防科大计算机学院学员一大队学员五队"
)

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!

= 绪论
== 课题背景及研究的目的和意义

=== 课题背景

无人机（Unmanned Aerial Vehicles）是指无人驾驶的飞行机器，被称为“空中机器人”。近年来，国内外无人机相关技术飞速发展，技术不断成熟，市场规模不断扩大。为了方便侦测与控制，无人机往往会搭载一颗或多颗摄像头，使其拥有极佳的视野。加之其卓越的机动性，无人机被广泛部署在交通监控、精准农业、灾害管理、工业生产和野生动物监控等领域。

2018年9月份，世界海关组织协调制度委员会（HSC）第62次会议决定，将无人机归类为“会飞的照相机”@droneglobal18，可见无人机在视觉方面的重要作用。相比传统路侧端的监控摄像头，无人机的效率与适应性都更强，对于推进计算机视觉（Computer Vision, CV）在生活中的应用起到了重要作用。

计算机视觉是人工智能（Artificial Intelligence, AI）领域的重要研究方向和分支。在最近十年间，计算机视觉领域发展飞速。计算机视觉技术利用计算机来模拟生物的视觉感官系统，通过计算机的处理，传感器输入的信号转变成计算机可以理解的信息，为其他下游任务提供视觉基础，比如：目标检测、图像识别、人体分析等等。计算机视觉因此也应用于生活方方面面，如人脸识别、交通监控、光学字符识别（OCR）和工厂生产监控等。

在计算机视觉领域中，最基础且最重要的问题之一就是目标检测（Object Detection）。其主要任务形式是在数字图像中识别并且分类出不同类别的视觉物体，包括感知实例、定位实例和分类实例。目标检测这一任务的目标是为计算机提供最基础的信息：物体在哪里？而目标检测中最重要的两个指标是准确率（包括分类准确率和定位准确率）和速度。目标识别也是其他下游CV任务的基础，如实例分割（Instance Segmentation）、图像字幕（Image Captioning）、目标追踪（Object Tracking）等。目标检测广泛应用于生活中，如自动驾驶技术、‌具身智能、工业检测‌、无人安防、智能医学等。

按照输出形式的不同，目标检测可以分为2D目标检测和3D目标检测。在过去几十年间，目标检测早期发展的主要是2D形式的检测算法，如AlexNet、YOLO、RCNN等模型。2D目标检测模型能够在图像平面中识别物体，例如识别出图片中的人体与动物等，可以检测实例的出现以及其在图像上的2D位置。然而由于RGB摄像头缺少对于深度信息的捕捉，无法将识别的物体对应到真实世界中，导致其难满足真实世界3D空间的需求。而3D目标识别算法利用传感器的数据来估计一系列详细的3D信息，如物体的3D大小、坐标、速度以及朝向等，极大地方便了与真实世界交互的需求。因此，3D目标识别在工业生产和学术研究中越来越重要。


=== 研究目的和意义

近些年随着无人机技术的飞速发展，无人机已被用于环境监测、行人交通监控、灾害疏散和工业生产等多个领域。@dronesreview19 这些应用都依赖于无人机系统上的计算机视觉模块来完成，而检测一个或多个相关的物体是计算机视觉的基础之一。因此，目标识别成为了几乎所有无人机系统的重要任务。

然而，现有的无人机应用的目标识别模型主要是针对传统的2D感知任务设计的，例如LAM-YOLO@LAM-YOLO24 和Drone-TOOD@Drone-TOOD24。2D图片只能提供2D平面上物体的数量和类别等信息，这限制了需要对环境进行3D理解的实际应用的发展。在基于视觉的机器系统中，3D感知扮演着重要的角色，其能够处理2D感知无法胜任的复杂任务。在无人机的相关应用中，鲁棒的目标识别对于无人机的有效部署至关重要。虽然对于无人机来说，3D视觉仍然是相对较新的技术，但它提供了在3D环境中捕获对象的完整维度数据的能力，使无人机在复杂极端情况下也能保持较高的鲁棒性。除了鲁棒性，3D信息的提供也让无人机的能力更加强大。无人机可以更加直观地获取到物体的相对3D坐标，配合无人机自身的坐标，可以快速、便捷地获取物体在真实世界上的3D位置，极大地强化了无人机的监控能力。

随着自动驾驶技术的发展，研究人员开始关注3D视觉下多角度的目标检测技术，出现了DERE3D@detr3d21、PETR@PETR22、PETRv2@PETRv2、BEVFormer@BEVFormer22 等车载3D多视角目标检测模型。这些模型能够通过车身的多个摄像头检测出车辆、行人等物体的3D信息，实时监控周边的交通情况，为自动驾驶提供了重要的3D基础信息。然而，目前尚未出现专注于无人机的3D多视角目标检测模型。同时，针对无人机3D多视角目标检测任务的数据集UAV3D@UAV3D24 日前发布，经过测试，先前的3D多视角目标检测模型在该数据集上表现均不理想，可能的原因是车载摄像头视角下车辆的与无人机视角下的车辆距离相差较大，模型难以成功识别远距离物体对象。这说明先前的研究结果并不能直接应用于无人机领域。如何研发适用于无人机端的3D目标检测模型对于交通管理、工业生产等领域都有着重要的意义和研究价值。

因此，本文将围绕无人机领域的3D目标检测技术深入研究，聚焦于提升无人机对于远距离物体的识别精度，旨在提升无人机对地面的感知能力，促进工业界的视觉产品发展。综上所述，本文的研究不仅具有较高的理论学术价值，同时具有广泛的实际工程应用前景。

== 国内外研究现状及发展趋势

本项目的研究目标位面向无人机的3D目标检测算法。我们将主要从无人机、目标检测算法、3D目标检测等几个方面介绍相关的工作。




=== 无人机

无人机因价格便宜、使用方便、对人员安全以及操作人员培训简单而越来越受欢迎。@onmonitoring18 这些优势，加上其的分辨率和强大的跟踪特性，促使它们在各种环境中的使用越来越多。无人机已被用于环境监测，包括空气污染、地表温度、洪水危险、森林火灾、道路表面损坏、地形监测、行人交通监测和灾害疏散。@dronesreview19 例如，许多人因为可以通过移动设备控制的先进产品而提高了生活水平。汽车技术通过提供关于交通的最新和精确信息来帮助驾驶员。无人机有多种规格、尺寸和配置。它们被归类为四大类别：固定翼、混合固定翼、单旋翼和多旋翼，同时考虑旋翼的数量。固定翼无人机适合于航空测量和绘图，因为它们稳定且续航时间长。混合固定翼无人机结合了自动化和手动滑翔，提供了可操作性和效率之间的平衡。单旋翼无人机虽然更复杂且成本更高，但为特定任务（如详细的地形测量）提供了卓越的精确度。最后，多旋翼无人机，尤其是四旋翼无人机，因其敏捷性、垂直起降能力和常用于监控和航空摄影应用而受到高度重视。多旋翼无人机可以是三旋翼、四旋翼、六旋翼或八旋翼。@aidrones24



=== 目标检测算法
// 这里用的 Object Detection in 20 Years: A Survey

// 目标检测对于人类甚至是普通生物都是较为简单的任务，很少会有人无法识别物品。然而在过去几十年间，如何让计算机学会检测各种图片中的示例却是一件很有挑战性的的任务。作为计算机视觉的基础任务之一，目标检测近一直是研究与应用的热点，并在几十年间迅速发展，诞生了许多目标检测算法与模型。依照使用的机器学习算法不同，我们将其分为传统目标检测算法和深度学习目标检测算法。

目标检测作是生物视觉系统与生俱来的核心能力。然而，在计算机视觉领域，自Marr提出视觉计算理论框架以来，实现类生物水平的通用目标检测一直是具有挑战性的任务上。为计算机视觉的基础任务之一，目标检测近一直是研究与应用的热点，并在几十年间迅速发展，诞生了许多目标检测算法与模型。根据特征生成机制的本质差异，现有方法论可划分为两大技术路线：
（1）基于显式特征工程的传统算法；（2）基于隐式表征学习的深度学习时期的算法。

=== 传统的目标检测算法

传统的目标检测算法主要兴起于上世纪的九十年代后期，主要使用特征工程和机器学习算法，如支持向量机（Support Vector Machine, SVM）、AdaBoost迭代算法和DPM（DeformablePart Model）以及梯度直方图特征（Histogram of Oriented Gradients,HOG）、局部二值模式（Local Binary Patterns，LBP）等。
传统目标检测算法的流程主要可以分为以下阶段：1.使用滑动窗口或者选择性搜索在图像中选取候选框；2.对每个框内的图像使用算子计算图像特征，得分超过阈值后标记候选框的位置；3.使用分类器对候选目标进行识别分类；4.最后对分类识别的结果进行一系列的后处理，比如说非极大值抑制（Non-Maximum Suppression ，NMS）来去除多余的候选框，来确定目标的最佳位置。
在传统的目标检测算法中，主要有三个里程碑式的算法：

// 我们介绍Viola Jones检测器@violaJones01，HOG检测器@hog05，基于部件的可变形模型（DPM）@dpm08。

Viola Jones检测器@violaJones01 是第一个检测速度能够达到实时检测的人脸检测器。
// VJ检测器主要由三个关键部件组成：Harr-like特征和积分图、级联分类器、AdaBoost算法。
大部分传统目标检测算法需要使用滑动窗口来对图像区域进行检测，然而大量候选区域成为了计算的瓶颈。Viola和Jones提出使用强分类器的级联作用快速筛选出不需要检测的区域，从而加快检测速度。而在计算区域特征时，VJ检测器使用Harr-like特征，这是一种卷积运算模板，需要大量的求和计算。因此VJ检测器引入了积分图算法，遍历一次图像后可以通过查表的方式计算任意矩形区域的像素和，免去了重复计算，大大提升了检测速度。然后通过AdaBoost算法迭代训练大量基础分类器，这些基础分类器的能力较弱，称为弱分类器。而把弱分类器联合在一起的性能表现会大幅提升，称为强分类器。将强分类器顺序组合在一起，形成级联结构。如果区域的计算值达不到弱分类器的阈值就会被排除，从而快速筛选区域。VJ检测器检测速度是当时算法的十倍甚至百倍，并且保持着同水平的正确率，成为目标检测算法的里程碑之一。然而，VJ检测器的的鲁棒性不足，很难识别部分遮挡的目标对象。

HOG检测器@hog05 在行人检测任务中首次实现了高精度与实时性的平衡。HOG检测器使用了方向梯度直方图（HOG）来提取图像的特征信息，HOG算子将网格的梯度方向信息进行统计，最后利用支持向量机对候选区域进行分类。HOG算子能更精准地描述物体的边缘和形状信息，对光照变换和局部变形都有着较好的鲁棒性。HOG检测器的这种策略旨在尝试平衡非线性与不变性，加强识别能力的同时又增强泛化性。尽管HOG检测器在行人检测等方面性能表现优秀，其对于复杂场景、旋转位移的处理存在不足。

可变形模型（DPM）@dpm08 吸收了HOG的思想，将基于手工特征的传统目标检测推向顶峰。DPM使用的特征是HOG特征的优化，其抛弃了HOG特征中块的概念，保留了单元的概念。并使用图像金字塔来采集HOG特征，从下到上特征从密集局部变成全局概略。DPM使用根滤波器（root filter）和部件滤波器（parts filter）实现全局综合评分与区域特征评分的计算，综合得分后成为区域的总体得分。DPM连续三届获取CVPR VOC的冠军，作者也被VOC授予终身成就奖，侧面体现出DPM的设计出色。然而，DPM只对刚性物体的检测效果好，但模型结构复杂，训练过程耗时，实时推理速度慢；超参数过多，优化参数流程繁多复杂。

传统目标检测算法高度依赖特征工程，其核心是研究者基于经典的图像处理理论（如边缘检测算子、梯度计算模型和纹理分析算法）构建特征提取范式。这类特征设计必须严格遵循人类可解析的数学公式或物理先验知识（如Canny边缘检测中的梯度阈值设定、HOG特征的方向梯度直方图统计），导致特征表征能力被限制在人类认知框架内，难以捕捉图像中复杂的抽象模式与跨尺度关联特性，最终难以适应开放场景下目标形态的多样性变化。受限于早期算力瓶颈，图像特征也只能构建浅层特征组合。即使是特征工程的巅峰时期，基于专家知识构建的复合特征也仅有$10^3$维，相较于目前深度学习动辄就能抽取$10^6$维以上的图像特征存在数量级上的巨大差异。

=== 深度学习时期的目标检测算法

早在1958年，F.Rosenblatt就提出了感知机模型;@perceptron58 到了1986年，Hinton等提出了基于Sigmoid激活函数的多层感知机模型与反向传播算法;@bp86 然而由于计算水平和数据水平的不足，深度学习未受到重视。直到2012年Hinton和他的学生设计的AlexNet@alexnet17 在ImageNet竞赛取得了跨越性的结果后，深度学习才真正获得了人们的关注，随后快速迭代演化。 自此以后，各种深度学习算法如雨后春笋般出现，在目标检测领域就有RCNN、Fast-RCNN、FPN、YOLO、DETR等算法。按照算法处理阶段的不同，我们将算法分为基于CNN的两阶段检测器和基于CNN的单阶段检测器。接下来我们将从这两个方面进行介绍。

* 基于CNN的两阶段检测器。*


// R-CNN的idea较为朴素：第一步通过选择性搜索策略提取一组潜在候选区域。然后将每个候选区域缩放为固定大小的图像，并输入预训练的CNN 模型（如 AlexNet）中以提取特征。最后使用SVM分类器预测每个区域内是否存在对象并识别对象类别。R-CNN在VOC07数据集@pascal-voc-2007 上有显著的性能提升，平均精度(mAP)从33.7%大幅提升至58.5%。

// 对大量重叠的候选区域进行冗余特征计算导致检测速度极慢，在使用GPU的情况下每张图像需要14秒。

2014年，R.Girshick等人首次将深度学习技术运用在目标检测领域，提出了带有CNN特征的区域（Region-CNN，R-CNN）@rcnn14 概念，R-CNN也是首个两阶段检测器。R-CNN的idea较为朴素：第一步，利用颜色、纹理、边缘等特征快速选取可能的目标区域；第二步，由于区域大小不同，将选取的的区域缩放成固定分辨率的图片；第三步，使用预训的CNN模型对图像计算特征；第四步，使用SVM分类器对图像特征进行分类，分为是否有目标以及对应的目标类型。RCNN成功在VOC07@pascal-voc-2007 实现了突破性的进展，将平均精度(mAP)从33.7%大幅提升至58.5%。R-CNN作为首个使用深度学习的目标检测算法，很难做到完美，有着许多的缺点，最大的缺点就是推理速度慢：对于第一步的所有可能的区域都要进行特征计算，而这些区域重叠率很高且数量庞大，导致推理速度远远达不到实时应用的标准。


// 同年，何凯明等人提出了空间金字塔池化网络(SPPNet)@sppnet15，以解决R-CNN速度慢的问题。以前的CNN模型仅能处理固定大小的输入，例如AlexNet只能输入大小为224x224的图像。SPPNet的主要贡献是引入了空间金字塔池化 (SPP) 层，这使得不管图像的大小如何，模型都能够生成固定长度的表示，无需重新缩放。使用SPPNet进行目标检测时，计算一次特征图便可以生成任意区域的固定长度表示，从而避免重复计算卷积特征。SPPNet比R-CNN快20倍以上，且没有牺牲检测精度，在其VOC07数据集上的mAP达到了更高的59.2%。SPPNet仍然存在一些缺点：首先，训练仍是多阶段的，导致过程繁琐复杂；其次，SPPNet简单地忽略了所有前面的层，只对全连接层进行微调，存在着很大的优化空间。

同年，何凯明等人关注到R-CNN工作的不足。为了解决大量重复计算导致R-CNN推理速度慢的问题，他们从图像区域输入尺寸进行创新，提出了空间金字塔池化网络（SPPNet）。在此之前，输入CNN的图片大小固定，而SPPNet提出的空间金字塔池化层（SPP）使得模型能够处理不同大小的图片并输出统一大小的特征。并且SPPNet解决了重复的区域特征计算，仅需计算一次整个图像的特征便可以获得任意区域的统一长度的特征。SPPNet在VOC07数据集上达到了比R-CNN更高的mAP指数（59.2%），同时检测速度是后者的20倍以上。表现出色的同时，SPPNet仍然有很多缺点：第一、在优化时，SPPNet简单的忽略了前面所有层，只对全连接层进行微调优化，存在很大的改进空间；第二、SPPNet是多阶段训练的，整个过程繁琐复杂。


// 2015年，R.Girshick等人在R-CNN和SPPNet的基础上进一步研究，提出了Fast R-CNN@fastrcnn15。Fast R-CNN能够在相同的网络配置下同时训练检测器和边界框预测器。在VOC07数据集上，Fast R-CNN将mAP从R-CNN的58.5%提高到了70.0%，同时检测速度比R-CNN快200倍以上。虽然Fast-RCNN成功地整合了R-CNN和SPPNet的优势，但仍然有改进空间，其检测速度受到候选区域生成速度的限制。

2015年，R.Girshick等人针对SPPNet需要多阶段训练的缺点进行了改进，提出了Fast R-CNN@fastrcnn15。Fast R-CNN 改进了卷积网络，提出了共享卷积特征计算并且引入了RoI Pooling。其能够在网络配置一样的情况下同时训练多阶段的模型，极大的提升了训练速度。同时，Fast R-CNN也表现出了优异的性能表现，速度达到了R-CNN的200多倍，同时在VOC07数据集上的精度达到了70.0%。尽管如此，Fast R-CNN仍有改进空间，其瓶颈在于候选区域的选取。在这之前，候选区域往往是使用滑动窗口生成的，数量庞大且生成速度较慢。意识到这个缺点后，我们很自然地能想到：“能用CNN模型快速生成候选区域吗？”接着，Faster R-CNN应运而生。


// Faster-RCNN 的主要贡献是引入了区域候选网络 (RPN)@fasterrcnn17，从而实现了几乎0成本的候选区域提取。Faster R-CNN  速度大幅提升，也是第一个检测速度接近实时的深度学习检测器。从R-CNN到Faster R-CNN，对象检测系统的大多数单个模块，例如候选区域检测、特征提取、边界框预测等，已逐渐集成到统一的端到端学习框架中。Faster R-CNN突破了Fast R-CNN的速度瓶颈，但在后续的检测阶段仍然存在计算冗余。后来，人们提出了各种改进，包括 RFCN@rfcn16 和 Light head R-CNN@lighthead17。 

2015 年，S. Ren 等人为了解决候选区域生成速度瓶颈的问题而提出了Faster R-CNN检测器@fasterrcnn17，其精度与性能表现都非常优秀。Faster R-CNN不仅在VOC07数据集和COCO数据集@coco15 上达到了73.2%和42.7%的mAP指标，，并且FPS（Frame Per Second）能够达到17的水平。Faster R-CNN的主要创新在于提出了专门用于生成候选区域的网络——区域候选网络（RPN），RPN能够快速且精准的生成候选区域，极大的提升了检测速度，使得Faster R-CNN也是第一个检测速度达到实时应用标准的目标检测算法。同时，Faster R-CNN也是第一个端到端的目标检测算法，目标检测中的大部分单个模块都被集成到一个框架中。同样，Faster R-CNN 仍有一些缺点，如计算的冗余。后续有人在Faster R-CNN的基础上进行改进，提出了RFCN@rfcn16 和 Light head R-CNN@lighthead17。 


// 2017 年，T.-Y.Lin等人提出了特征金字塔网络FPN@fpn17。在FPN之前，大多数基于深度学习的检测器仅在网络顶层的特征图上进行目标检测。深层次的图像特征蕴含了较多类别信息，却难以提取位置信息。为此，FPN使用了一种具有横向连接的自上而下的架构，能够在所有尺度上构建高级语义。由于CNN通过其前向传播自然形成特征金字塔，FPN在检测各种尺度的物体方面具有显著的优势。在基本的Faster R-CNN模型中使用FPN，它在COCO数据集上实现了当时最先进的单模型检测精度。现在FPN已成为许多最先进的检测器的基本模块。

2017年，针对当前目标检测算法难以识别不同尺度目标的问题，T.-Y.Lin等人提出了一种多尺度的目标检测算法——特征金字塔网络（FPN）@fpn17。他们发现，不同层次的特征图偏向的特征信息不同：低分辨率的高层次特征图蕴含比较丰富的语义信息，而高分辨率下的低层次特征图对于目标的位置信息感知清晰。先前的算法均使用最高层的特征图，导致对于小目标的检测效果较差。FPN利用卷积神经网络在前项过程中天然形成的特征图作为特征金字塔，然后自上而下逐步融合语义信息与空间位置信息，并使用横向连接进行融合，各层次融合的特征图提取区域合并后再进行检测。FPN的效果非常好，并且兼顾了推理性能，现在FPN已经是目标检测模型的基本模块之一。


* 基于CNN的单阶段检测器。*

两阶段检测器遵循经典的级联处理流程。第一阶段以提高召回率为核心优化目标，注重于生成候选区域；第二阶段基于候选区域提取特征，通过分类与回归网络（Classification & Regression Head）实现细粒度边框定位与判别性特征学习。此类方法凭借两阶段任务解耦的特性，无需复杂的后处理机制就可以实现较高的检测精度。然而，双阶段串行处理的计算范式导致推理速度受限，难以满足工业场景中的低延迟、高吞吐的需求。相比之下，单阶段检测器采用密集锚点或中心点策略，通过单次推理便可以直接输出目标的位置与类别。得益于该轻量化设计，单阶段检测器在各种移动设备上具有较大的优势。

在单阶段目标检测算法的发展历程中，YOLO（You Only Look Once）系列模型具有里程碑意义。由Redmon等人于2015年提出的初代YOLO@yolo16，首次将网格化预测机制（Grid-based Prediction）引入目标检测领域，其核心思想是通过单次前向传播完成图像全域的边界框回归（Bounding Box Regression）与类别概率预测（Class Probability Estimation）。该模型在PASCAL VOC 2007数据集上实现52.7% mAP\@0.5的同时，推理速度达到45-155 FPS，显著超越传统两阶段方法。然而，YOLO的空间分离约束（Spatial Separation Constraint）导致其对密集目标和小物体的定位精度不足，这一问题在后续迭代的SSD与YOLO自身改进版本中得到针对性优化 。2025年2月，YOLO最新版本YOLOv12发布@yolov12 。这一版引入了Attention机制，使其能够在提升精度的同时保持极高的推理速度（YOLOv12-N的单图像延迟为1.65ms），可见YOLO系列模型的学术生命力之鲜活。

单次多框检测器（Single Shot MultiBox Detector, SSD）由Liu等人于2016年提出@ssd17，其创新点在于多尺度特征金字塔（Multi-scale Feature Pyramid）与预定义锚框（Predefined Anchors）策略的结合。通过在不同层级特征图上部署不同长宽比的锚框，SSD有效提升了小目标检测的召回率，在COCO数据集上达到46.5% AP\@0.5，快速版本推理速度达59 FPS。相较于YOLO的单一尺度预测，SSD的层级特征融合机制成为后续模型设计的基准范式。

尽管单阶段检测器在速度上占据优势，但其精度长期落后于两阶段方法。Lin等人于2017年通过类别不平衡理论分析（Class Imbalance Analysis）揭示根本原因：训练过程中大量简单负样本主导梯度更新，导致难样本学习不充分@lin2017focal。为此RetinaNet引入了焦点损失函数（Focal Loss），通过动态调整难易样本的损失权重，使模型在COCO数据集上实现59.1% AP\@0.5，首次达到与两阶段方法相当的精度水平。

基于关键点检测的创新中，Law等人提出的CornerNet@law2018cornernet 摒弃了传统锚框设计，转而通过角点热力图（Corner Heatmaps）与嵌入向量匹配（Embedding Matching）实现边界框生成，在COCO数据集上取得57.8%的AP\@0.5。该方法的无锚点（Anchor-free）特性显著降低了超参数调优难度，但角点分组依赖的后处理步骤仍存在计算冗余。Zhou等人进一步优化的CenterNet将目标建模为中心点热力图（Center Heatmaps），通过中心点直接预测物体的尺度与位置，在消除非极大值抑制（NMS）后处理的同时，将精度提升至61.1% AP\@0.5，展现了端到端检测框架的潜力。

Transformer架构的引入标志着目标检测进入全局注意力驱动（Global Attention-driven）的新阶段。Carion等人于2020年提出的DETR首次实现完全端到端检测，通过集合预测（Set Prediction）机制和编码器-解码器架构（Encoder-Decoder Architecture）替代手工设计的锚框与NMS。然而，其平方级计算复杂度（O(N²) Complexity）导致训练收敛缓慢且小目标检测性能受限。Zhu等人提出的Deformable DETR引入多尺度可变形注意力（Multi-scale Deformable Attention），在COCO数据集上以71.9% AP\@0.5 刷新性能记录，同时将训练周期缩短至原算法的1/10 。

=== 基于RGB图像的3D目标检测算法 <1.2.5>
// 这里用的文章是 3D Object Detection for Autonomous Driving: A Survey


2D目标检测一定程度上促进了3D目标检测的发展。3D目标检测方法可以分为单视角和多视角两个方面。
@odsurvey22

*基于单视角的3D目标检测方法*

单视角3D目标检测方法思想大多源于二维检测框架，通过单目或立体图像直接推理目标三维属性，其主流技术路径可归纳为三类：基于模板匹配的候选框生成、基于几何约束的姿态推导以及基于伪激光雷达的跨模态迁移。

基于模板匹配的方法以穷举采样为核心策略，典型代表如Chen等人提出的3DOP@3dop15，通过立体图像深度估计构建三维点云，将候选框生成转化为马尔可夫随机场（MRF）的能量最小化问题，依赖人工设计势函数（如地平面连续性约束）优化空间分布，最终通过Fast R-CNN@fastrcnn15 实现目标定位；Mono3D@mono3d16 则针对单目相机场景，采用滑动窗口在三维空间采样候选区，假设地平面正交于图像平面以降低搜索复杂度，但需依赖语义分割过滤无效区域，其过度工程化设计导致复杂场景泛化性受限。

基于几何属性的方法摒弃冗余候选生成，转而利用二维检测框的几何特性推导三维姿态：Deep3DBox@3dbox17 强制三维角点透视投影与二维框边缘对齐，通过可微几何约束优化边界框参数；Li等人提出的GS3D@gsd3d19 在Faster R-CNN@fasterrcnn17 框架上引入方向预测分支，联合二维框与粗略三维框的可见面纹理特征进行融合，通过表面特征消除视角歧义，但其依赖经验性几何假设（如目标中心点投影与二维框顶部对齐），在目标尺度突变时引发误差累积。

基于伪激光雷达的方法通过单目深度估计构建伪点云，然后利用现有的点云目标检测框架：Xu等人的MF3D@mf3d18 融合RGB图像与视差图生成的前视图特征，通过多级特征拼接增强小目标检测能力；Weng等人提出的Mono3D-PLiDAR@mono3d-plidar19 引入2D-3D边界框一致性损失（BBCL）约束空间对齐，并利用Mask R-CNN@MaskR-CNN20 的实例掩码剔除截锥体外噪声点，但单目深度估计的固有误差（如距离相关的深度伪影）仍限制其远距离检测精度。总体而言，单视角方法虽在实时性与硬件成本上占据优势，但其性能瓶颈仍源于几何先验的强假设与跨模态表征的次优解耦，这驱动研究者向多传感器融合与端到端联合优化方向持续探索。


*基于多视角的3D目标检测方法。*
// 这里基于https://zhuanlan.zhihu.com/p/686868635

// 这些方法首先将多幅图像转换成前视图或鸟瞰图(BEV)展示，在网格中密集以利用 CNN 和标准 2D 检测方案。
多视角的3D目标检测方法大都会转换目标的特征空间，将其从多视图转换为前视图或者鸟瞰图（Bird Eye View）。转换特征空间后使用其他3D检测框架获取目标信息。

*转换为前视图特征*。DETR3D@detr3d21 由Wang等人提出，该算法是DETR在三维目标检测领域的延伸，它通过几何反投影和相机变换矩阵将二维特征提取与三维目标预测联系起来，实现了无需密集深度估计的三维目标检测。DETR3D将多视图检测问题转化为集合到集合的预测任务，通过预设的object queries和神经网络解码出三维空间中的参考点，再将这些点反投影到二维特征图上，通过双线性插值采样特征值，最终通过多头注意力机制和Transformer@transformer17 解码器来优化queries并预测边界框和类别。然而DETR3D存在预测参考点不准确、无法从全局角度进行表示学习等缺点，为此，Liu和Wang等人提出了PETR@PETR22。PETR通过引入3D坐标生成器和3D位置编码器，将三维坐标的位置信息编码为图像特征
// ，从而实现多视图三维目标检测
。
PETR首先在三维空间中选取均匀分布的锚点，接着通过全连接网络生成初始3D的Query。与DETR3D学习Query初始化3D位置不同，PETR从三维空间的点编码后称为Query，这样做避免了Query映射回图像后找不到对应点的问题，并实现了在三维特征空间中的训练。之后Liu等人提出了PETRv2@PETRv2，PETRv2的创新点在于引入时序逻辑，通过将前一帧的特征与当前帧的特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。PETRv2通过特征引导的位置编码器将图像特征和三维位置信息结合，隐式引入了视觉先验，提高了模型的性能。

*转换为鸟瞰图（BEV）特征*。Huang等人提出的BEVDet@BEVDet22 是另一种高性能的多相机三维目标检测方法，它在鸟瞰图（BEV）空间中进行目标检测。BEVDet面临过拟合问题，因为它在BEV空间下过度拟合。为了解决这个问题，BEVDet应用了定制的数据增强策略和尺度NMS（Scale-NMS），以改进算法的泛化能力和检测精度。
之后Huang等提出BEVDet4D@BEVDet4D22，BEVDet4D通过将前一帧的特征与当前帧中的相应特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。Li等人提出的BEVFormer@BEVFormer22 利用可变形注意力机制设计了空间交叉注意力和时间自注意力，分别从跨摄像机视图的感兴趣区域提取空间特征和循环融合历史BEV信息，从而实现对三维场景的理解和目标检测。



== 研究内容以及技术路线

针对目前无人机3D目标检测领域研究空白的问题，本文提出了一种多尺度时空融合的端到端3D目标检测算法，旨在提升目前无人机的3D感知能力。本文通过对现有车端3D目标检测算法进行分析，发现现有算法对于遮挡目标和小目标的检测能力较差，提出了跨帧Query传递机制、运动补偿机制和引入多尺度特征融合算法。提出的方法在公开的UAV3D数据集上进行实验，证明了改进算法的有效性。本文的主要贡献如下：

（1）针对DETR3D模型在UAV3D数据集上对于半遮挡和遮挡物体的检测效果差的问题，提出了跨帧Query传递机制和运动补偿机制。本文提出的跨帧Query传递机制可以合理利用时序信息，将时序信息与空间信息融合，提升模型对于时空的感知能力，以提升模型对于半遮挡和遮挡物体的感知能力。

（2）针对DETR3D模型在UAV3D数据集上对于小目标物体难以识别的问题，引入了特征金字塔网络FPN。通过ResNet-101前项传播的过程形成特征金字塔并融合多尺度信息，提升模型对于小目标的感知能力。

最后，在UAV3D数据集上对各个模块进行了验证，最终改进算法相较于DETR3D在mAP指数上提升了16.0%，NDS指数提升10.6%。

改进模型结构如下：
#{
set text(size: 10.5pt)
figure(image("images/整体框架.svg"), caption: "改进模型结构")
}
\




= 相关基础知识介绍

== 引言

在前文中已经介绍了目标检测，其作为计算机视觉的基础任务之一，拥有重要的研究价值。然而，由于计算资源和传感器的发展受限，目标检测的早期研究注重于2D目标检测任务。2D目标检测的任务难度较低，数据集制作也更为方便。2D目标检测数据集仅需要单目相机，目标标注也只涉及图像上的2D信息。
与之相比，精确的3D目标检测需要获取对象相较于传感器的3D位置信息，对于算法的空间建模能力要求高，是人工智能领域的难题之一。尤其困难的是数据集的收集与标注，不仅需要使用多个或多种传感器来收集3D信息，还需要对目标对象进行精确的3维坐标标注，导致3D目标检测数据集制作成本高昂。

不过随着增强现实(Augmented Reality,AR)、自动驾驶和其他机器人导航系统等技术的应用，3D目标检测被推动着快速发展。在这些应用中，如自动驾驶，需要算法彻底了解周围环境，不仅是物体的种类，还要求准确获取其姿态与朝向，来规划路线以避免碰撞，这正是3D目标检测擅长的。

随着车端开始从2D目标检测发展3D目标检测，作为无人系统的无人机也需要将传统的2D目标检测转向3D目标检测。但目前专门适用于无人机的3D目标检测数据集极其稀少，并且同时也鲜有人研究面向无人机的3D目标检测算法。接下来，我们将首先介绍无人机视角的3D目标检测数据集的选取与分析；然后我们将深入探究Transformer的基本原理，并且介绍端到端的3D目标检测模型DETR3D；最后我们介绍3D目标检测的性能评价指标。



== 数据集选取与分析

无人机目标检测数据集有很多，包含许多的目标对象，如VisDrone 数据集@zhu2018vision、UAVDT 数据集@du2018unmanned、ITCVD数据集 @dans-xnc-h2fu_2018、UCAS-AOD数据集@zhu2015orientation 等等，然而这些数据集均为2D目标检测任务，无法使用。比较著名的3D目标检测数据集有KITTI数据集@geiger2013vision 、Waymo Open 数据集@sun2020scalability、NuScenes数据集@caesar2020nuscenes、Appllo Scape数据集@wang2019apolloscape 等数据集。不过这些数据集面向自动驾驶任务，采集的数据均为车端数据。目前，仅有UAV3D数据集为唯一开源的无人机3D目标检测数据集。



#{
set text(size: 10.5pt)
figure(image("images/Chapter2/KITTI.png")+ image("images/Chapter2/NuScenes.png"), caption: "KITTI数据集（上）和NuScenes数据集（下）")
}

=== 数据收集部分

由于在高空中目标的3D坐标以及其他信息难以测算和校准，UAV3D数据集使用CARLA模拟器@dosovitskiy2017carla 来模拟车流环境并记录数据，并使用AirSim模拟器@shah2018airsim 来模拟无人机飞行。在CARLA中，车辆生成后会随机导航到穿过城镇。在数据集中一共有3、6、7和10号4个城镇，这四个城镇都有着复杂的交通情况，有着许多红绿灯路口和T型路口。在每个城镇记录250个场景（Scene），总共1000个场景。

为了充分模拟复杂的飞行场景，UAV3D数据集4个城镇分为Urban（3，10号城镇）与SubUrban（6，7号城镇）。在CARLA模拟器中，10号城镇因为交通场景复杂且拥塞而出名，6、7号城镇车辆则相对稀少，这种城镇与郊区的均衡还体现在行人、建筑、车辆与道路标记上。对于每个城镇，无人机都有25条线路来尽可能地覆盖整个城镇区域。

UAV3D数据集面向3D目标检测任务，其在无人机上设置了不止一个摄像头。在无人机的前后左右和底部分别有一个摄像头，以保证无人机有足够的感知范围。四周的摄像头倾角为-45°，底部的摄像头则为水平放置，提供鸟瞰（BEV）图像。每个摄像头拍摄图像的分辨率为800×450像素。CARLA模拟器使用UE坐标系（即左手坐标系），x轴朝前，y轴向右，z轴朝上。但AirSim使用NED坐标系（North East Down），NED坐标系是导航领域常用的坐标系，三个轴分别指向地球北极、东方向和地心。在UAV3D中，作者将传感器的坐标从AirSim坐标转换为UE坐标系，以方便后续的对齐工作。

多无人机协同也是无人机的研究热点之一，UAV3D数据集为此添加了无人机纵队，一共五台无人机，位置分别为前后左右以及中间，四周的无人机与中间无人机的距离为20米。整个无人机纵队在60米的高中进行感知和协同任务。




#{
set text(size: 10.5pt)
grid(
  columns: 2,
  column-gutter: 2em,
  row-gutter: 1em,
  figure(
    image("images\Chapter2\UAV3d_uav_structure.png", height: 4.82cm),
    caption: [传感器于无人机上的分布]
  ),
  figure(
    image("images\Chapter2\UAV3d_uav_cross.png",  height: 4.82cm),
    caption: [无人机纵队示意]
  )
)
}


=== 数据标注及与其他数据集的对比

UAV3D将5架无人机的5个摄像头同时记录下来作为一个样本（sample），即一个样本有25张图像。UAV3D从CARLA模拟器中获取车辆的3D信息作为标注，从AirSim模拟器中获取相机的内存与外参矩阵提供给下游任务。UAV3D的标注信息包括目标的3D边界框以及像素级别的语义分割标签。每个3D边界框包含物体中心坐标（x,y,z）、边界框的长宽高以及物体的朝向角（yaw,pitch,roll）。前文提到UAV3D有1000个Scene，每个Scene包含20个sample，总共有50万张图片和330万个3D边界框。这些数据被分为训练、验证与测试三个部分，格式与使用较多的nuScenes数据集相似，可以直接使用nuScenes-devkit工具库进行读取加载。

下表为UAV3D数据集与其他数据集的对比。V2X、V2V和V2I分别表示设备对万物、设备对设备和设备对基础设施的合作，C、 L和R分别指摄像头、激光雷达和雷达传感器。

#{
figure(
table(
  columns: (58pt,auto,auto,42pt,auto,auto,auto,auto,auto,auto,auto),
  rows: (15pt, 18pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt), 
  align: center,
  // inset: 5pt,
  table.header(
    [数据集], [年份], [来源], text(size: 7.8pt)[应用场景], [V2X],[模态],[场景], [帧数], [图数], text(size: 9pt)[标注数], [类别]
  ),
  [VisDrone @zhu2018vision], [2018], [真实], [无人机], [无],[C] ,[–], [18万], [1.02万], [无], [10],
  [UAVDT @du2018unmanned], [2018], [真实], [无人机], [无],[C],[–], [8万], [8万], [无], [3],
  [Waymo @sun2020scalability], [2019], [真实], [驾驶],[无],[C&L],[1000], [20万], [100万], [120万], [4],
  [nuScenes @caesar2020nuscenes], [2019], [真实], [驾驶], [无],[C&L&R],[1000], [4万], [140万], [140万], [23],
  [OPV2V @xu2022opv2v], [2022], [模拟], [驾驶], [V2V],[C&L],[–], [–], [4.4万], [23万], [1],
  text(size: 9.81pt)[V2X-Sim @li2022v2x], [2022], [模拟], [驾驶], [V2X],[C&L],[–], [–], [6万], [2.66万], [1],
  [V2XSet  @xu2022v2x], [2022], [模拟], [驾驶], [V2X],[C&L],[–], [–], [4.4万], [23万], [1],
  text(size: 8.2pt)[DAIR-V2X @yu2022dair], [2022], [真实], [驾驶], [V2I],[C&L],[–], [–], [3.9万], [46.4万], [10],
  text(size: 5.4pt)[CoPerception-UAV @hu2022where2comm], [2022], [模拟], [无人机], [V2V],[C],[183], [0.44万], [13.2万], [160万], [21],
  [V2V4Real @xu2023v2v4real], [2023], [真实], [驾驶], [V2V],[C&L],[–], [–], [4万], [24万], [5],
  [Rcooper @hao2024rcooper], [2024], [真实], [驾驶], [V2I],[C&L],[–], [–], [5万], [–], [10],
  text(size: 6.68pt)[TUMTraf-V2X  @zimmer2024tumtraf], [2024], [真实], [驾驶], [V2I],[C&L],[–], [–], [0.5万], [2.93万], [8],
  [HoloVIC @ma2024holovic], [2024], [真实], [驾驶], [V2I],[C&L],[–], [10万], [–], text(size: 9.8pt)[1140万], [3],
  text(size: 9.3pt)[V2X-Real @xiang2024v2x], [2024], [真实], [驾驶], [V2X],[C&L],[–], [–], [17.1万], [120万], [10],
  text(weight: "bold")[UAV3D], [2024], [模拟], [无人机], [V2V],[C],[100], [2万], [50万], [330万], [17]
),
caption: [UAV3D与其他数据集 ],
)

}

== Transformer基本原理

在Transformer之前，许多先进模型的都依赖于RNN。尽管出现了LSTM、GRU等门控RNN，RNN对于长序列的记忆能力仍然不足，句末的单元往往缺少前端标记的精确信息。同时，RNN的每个单元都依赖于序列靠前单元的结果，导致RNN无法并行处理，训练效果底下。随着谷歌团队在《Attention is All You Need》@vaswani2017attention 论文中提出注意力机制（Attention Mechanism），这些问题都得到了解决。而Transformer结构也证明注意机制足够强大，Transformer也逐渐成为了现今最流行的深度学习模型。

=== 整体结构

Transformer的整体结构主要分为左右两端（如@transformer_fig），左边为编码器（Encoder），右边为解码器（Decoder）。一层Transformer由一个编码器和解码器组成，而Transformer可以堆叠多层以提升模型的能力，而模型的训练难度也随之成倍增加。在《Attention is All You Need》原文中，作者权衡训练成本与效果最终使用了六层结构的Transformer。

输入的序列在经过Embedding后进入编码器，并将处理后的序列传递给下一层的编码器以及同层的解码器。同层的解码器将输入的某个特定序列和编码器的输出一起处理并传递到下一层的解码器。直到最后一层的解码器得到输出并使用其他的神经网络如MLP来得到最终的结果。

接下来我们将从注意力机制开始逐步介绍Transformer。

#{
  figure(
    image("images\Chapter2\Transformer_chinese.svg",height:10cm),
    caption: [Transfomer 结构]
  )
}<transformer_fig>





=== 多头注意力机制 <attention>

注意力机制如其名，灵感来源于人类的注意力。大脑在处理信息时，往往会集中关注某些区域，这些区域往往是信息中关键的部分，过滤掉无用的信息，从而提升信息的处理速度和精度。这种信息处理策略被称为注意力机制。以此为启发，研究员提出了神经网络的注意力机制，广泛运用于各种深度学习模型中，取得了巨大的成功。本小节我们主要介绍Transformer中的自注意力机制（self-attention）。

// 自注意力机制是一种特殊的注意力机制，对于序列本身进行注意力计算，给不同的元素分配不同的权重以获取序列内部的联系。自注意力机制的核心思想是学习映射来查询向量Q（Query）、K（Key）、V（Value）之间的权重关联，进而构建全局的的关联权重。每个序列中的单元与该序列中的所有单元进行注意力计算，自适应地学习到输入序列中的关键信息，并自动捕获不同子空间间的相互关系。

自注意力机制与其他注意力机制不同，其在计算关注的部分时利用本身进行计算，得到的权重结果代表着序列元素之间的某种相关性，因此其对于输入序列的顺序并不敏感。自注意力的本质思想是一种查询和映射，Query通过Key来查询Value以获取其注意力权重，并将所有权重使用Softmax归一化得到权重百分比。自注意力机制最大的特点在于Query、Key、Value均来自于输入的序列，权重结果关注于序列内部之间的关系，其他的注意力机制Query、Key、Value的来源则不完全相同。

#{
figure(
  image("images\Chapter2\QKV.png",height: 3cm),
  caption: [Self-Attention机制的本质思想]
)
}
#let attention = math.op("Attention")
#let softmax = math.op("softmax")

在Transformer中，输入的序列会在线性映射后加入预设好的位置编码，形成自注意的输入序列$x$。通过三个不同的转换矩阵$W_q$，$W_k$,$W_v$转换为不同的输入序列：Query Token、Key Token、Value Token，即为Q、K、V。自注意机制使用缩放点积注意力（scaled dot-product attention），Query查询Key后得到一个注意力权重$Alpha$,其中$alpha_(i,j)$为$q_i$和$k_j$的点乘。之后将$a_(i,j)$除以$sqrt(d_k)$来增强训练的鲁棒性，并使用softmax函数来将权重归一化。这一系列计算可以使用矩阵来表示，即

$ attention(Q,K,V) = softmax((Q  K^T )/sqrt(d_k) ) V $

其中 $ Q = X dot W_q ; K = X dot W_k ; V = X dot W_v $
矩阵化极大的加速了自注意力机制的运算，使其有着极大的加速空间。


为了增强自注意机制对于多方面信息的思考能力，让其对于逻辑语义能够有更加细腻全方面的思考，Transformer中加入了多头自注意力机制。我们可以将自注意力机制中的$W_q,W_k,W_v$矩阵看做一个读取头，其中的参数是模型对于输入$X$的一种理解。如果我们加入$n$个$W_i^q,W_i^k,W_i^v$矩阵，这样模型便拥有了$n$个头，每个头都能够对$X$产生某个方向的理解。最后多头注意力机制将所有头产生的结果综合，生成最后的结果。公式表达如下
#let multihead =  math.op("MultiHeadAttention")
#let concat = math.op("Concat")
$
 multihead(Q,K,V) = concat("head"_1, dots,"head"_h)W^O\
 "where" "head"_i = attention(X W^Q_i,X W^k_i,X W^V_i) \
  \( W_i^Q in bb(R)^(d_(m o d e l) times d_k) , W_i^K in bb(R)^(d_(m o d e l) times d_k) , W_i^V in bb(R)^(d_(m o d e l) times d_v) , W_i^O in bb(R)^(h d_v times d_(m o d e l)) \) 
$



=== 编码器

Transformer使用了Seq2Seq（sequence to sequence，序列到序列）的经典架构：编码器-解码器（Encoder-Decoder）。编码器的主要作用是将输入的序列编码成同样长度的序列，交由解码器处理。
#{
  figure(
    image("images\Chapter2\EncoderChinese.svg",height:8cm),
    caption: [Encoder 结构]
  )
}
Encoder首先对原始序列$X$进行“预处理”：首先使用词嵌入（Word Embedding）的方式对X进行编码，将X中的每个Token转换为对应的向量，这种方式可以更好的表征Token；然后引入位置编码（Positional Encoding），自注意机制没有采用RNN结构，使用全局信息，但弊端是无法利用Token的顺序信息。位置编码的引入使得自注意力机制能够利用Token的顺序信息，其计算公式如下
#let PE = math.op("PE")
$ 
  PE("pos",2 i) = sin("pos /"10000^(2 i"/"d)) \ 
  PE("pos",2 i+1) = cos("pos /"10000^(2 i"/"d))
$

处理好后的序列X进入多头自注意力模块，其机制在上一小节介绍过。多头自注意力模块的输出后经过Add & Norm层。其中计算公式如下
#let LayerNorm = math.op("LayerNorm")
$ X = LayerNorm(X + multihead(X)) $ 
其中Add代表X与多头注意力模块的结果相加，本质上是一种残差网络。残差网络可以有效解决深层网络训练时的梯度消失和梯度爆炸问题，而Norm是一种正则化，为层正则化（Layer Normalization），能够将每一层神经元的输入转成相同的均值方差，可以加速训练时loss的收敛。

经过多头注意力模块和Add & Norm层后是前馈层（Feed Forward），结构较为简单，为两层的全连接层，第一层使用ReLU激活函数，第二层无激活函数，计算公式如下：
$ X = max(0,X W_1, + b_1)W_2 + B_2 $
之后再是一层Add & Norm层，这样便组成了一个编码器块（Encoder Block）。



=== 解码器与损失计算

解码器将编码后的序列$X$进行解码，得到最终的输出序列。Transformer的解码器是一个自回归解码器，其与编码器的主要区别为中间加入了额外一层编码器-解码器注意力(Encoder-Decoder Attention，也叫Cross Attention)，即编码器与解码器的连接点，。

#{
  figure(
    image("images\Chapter2\DecoderChinese.svg",height:10cm),
    caption: [Decoder 结构]
  )
}

编码器-解码器注意力与普通的多头自注意力模块的不同点在于$K,Q,V$的来源不同。在@attention 中我们详细介绍了多头自注意力机制，其中自注意力机制的$Q,K,V$均来源于输入$X$。而在编码器-解码器注意力中，$Q,K,V$的来源不一致，其中$K,V$来自于编码器的输出，而$Q$则是编码器经过掩码多头自注意力模块得到的。计算公式表示如下
$
  X_("decoder")^' = softmax(W_Q X_("decoder")W_K X_("encoder")) W_V X_"encoder"\
  X_("decoder")^' = LayerNorm(X_("decoder") + X_("decoder")^')
$

值得一提的是解码器的掩码多头自注意力模块（Masked Multihead Attention）。由于解码器预测序列是逐个输出，所以解码器不能使用当前Token位置之后的信息，需要在计算的过程中遮盖对应部分。因此编码器引入了掩码操作（Mask），在计算注意力分数时（即$Q dot K^T$）使用掩码矩阵将后序时间步上的Token信息置为空。
#{
  figure(
    image("images\Chapter2\MaskAttention.png",height:3cm),
    caption: [MaskAttention]
  )
}

其余部分与编码器相同，最后解码器的输出经过一层线性层变换后使用Softmax得到了每个Token对应的概率，取最大值即为结果。

不同的下游任务决定了训练Transformer使用的损失函数，在论文原文中任务为预测任务，所以使用的为交叉熵损失（Cross Entropy Loss）。

== 端到端3D目标检测模型DETR3D

DETR3D模型基于DETR模型改进而来，将端到端检测框架带入3D目标检测领域，其通过query来预测目标在3D空间的位置。由于不需要进行深度估计而避免了复合误差的影响，这种自上而下的方法优于自下而上的方法。并且DETR的端到端框架不需要非极大值抑制（NMS）等后处理，极大的提高了推理速度。

=== 端到端目标检测框架 DETR

DETR@carion2020end 模型使用了一种全新的视角来看待目标检测任务，是首个将目标检测重构为集合预测任务的端到端模型。其思路是将目标检测看作一个集合预测的问题：预测目标图片中的边界框的集合。DETR利用Transformer模型，将集合预测的任务巧妙转换成seq2seq（sequence to sequence，序列到序列）形式。DETR的核心架构由三部分组成：1、特征提取骨干网络；2、Transformer的编码器-解码器架构；3、集合预测损失函数。我们依次介绍这三个部分。

#figure(
  image("/images/Chapter2/DETR.png",height: 3cm),
  caption: [DETR模型结构图]
)

#text(weight: "bold")[特征提取骨干网络]。将输入的图片提取特征，是计算机视觉中常用的做法。DETR使用CNN来提取图片的多尺度特征图（如经典的ResNet）,抽取出的特征不但减少了模型的计算量，还加速模型训练时的收敛速度，并且在不同下游任务中拥有优异的泛化性。

*Transformer编码器-解码器架构*。特征提取骨干网络抽取的特征展平（flatten）后加上位置编码即为Query序列。Query序列经过多层编码器编码后传递给解码器，解码器与编码后的序列做交叉注意力后输出结果序列。结果序列经过简单的前馈神经网络后得到分类的结果。这里值得一提的是编码器的初始化输入序列是可以学习的参数，每一个Query代表着一个物体，称为物体序列（Object Query)，经过交叉注意力对特定物体进行聚合得到物体的精细位置。

*集合预测损失函数*。DETR将目标检测看作集合预测问题，其优化目标即输出集合与真实集合一致。这一优化目标需要模型预测全局的目标整体。为了将集合中的元素一一对应，真实集合中会加入空集元素代表无目标（DETR的预测元素个数固定且远大于真实目标个数），然后使用二分图匹配算法（匈牙利算法）来计算两个集合之间的最佳匹配，该匹配下的定位损失和分类损失作为评判的标准，即为损失函数。


=== 多视角端到端3D目标检测模型 DETR3D

// DETR3D将DETR中基于Transformer的2D检测框架引入到了3D检测任务中：一次性生成N个bbox，采用set-to-set损失函数计算预测和GT的二分图匹配损失。这种方式避免了常规3D检测任务中所需的深度估计模块，因此无需集中算力进行冗余信息的处理，而只关注在目标的特征之上，速度得到了较大的提升，也避免了重建带来的误差。此外，DETR3D也无需NMS等后处理操作。

DETR3D在DETR3D的基础上进行拓展，保留了DETR的端到端检测框架，而引入了3D空间与多视角图像平面的特征提取。DETR有编码器与解码器，而DETR3D仅有解码器。DETR3D利用解码器的交叉注意力将Query中蕴含的3D坐标与图像中的特征进行计算，修正坐标位置，最后与DETR一样使用set-to-set损失函数。DETR3D同样可以分为3个部分：

*特征提取骨干网络*。
// 输入车载环视的6张图片,每张图片通过ResNet等2D骨干网络提取特征；再通过FPN得到4个不同尺度特征图
DETR3D利用ResNet来提取输入的多视角图像的2D特征，这些特征随后用于解码器的注意力部分。

#figure(
  image("/images/Chapter2/DETR3D.png",height: 3cm),
  caption: [DETR3D模型示意图]
)<detr3d_fig>

*解码器*。
解码器初始化物体序列的生成类似DETR，随机生成$M$个Query。经过自注意力模块后，在解码器的交叉注意力部分，如@detr3d_fig 中蓝线所示，使用神经网络将Query转换为3D空间的一个点。然后如绿线所示，利用相机内外参，将3D空间中的点投影回2D图像平面中。

// 由于投影点经过下采样后在不同尺度的特征图上很可能没有刚好对应的特征点，因此采用双线性插值的方法来获取得到在每个尺度上的特征。将不同尺度上和不同位置相机上的提取到的特征进行求和平均处理，利用多头注意力机制，将找出的特征映射部分对物体序列进行修正。这种修正过程是逐层进行的，理论上，更靠后的层应该会吸纳更多的特征信息。
这种投影很可能没有恰好对应的像素点（位置是整数），所以可以采用线性插值来获取特征。获取2D平面的特征后进行平均，并作为交叉注意力的Key与Value进行计算。交叉注意力模块的意义在于使用图像特征对Query的坐标进行修正，由于解码器有多层，所以Query的坐标也随之逐渐接近真实坐标。


*集合预测损失函数*。这一部分与DETR相似，解码器的每一层输出都计算loss。回归损失采用L1损失，分类损失使用focal loss。




== 算法性能评价指标与方法

本项目的任务为3D目标检测，广泛运用的评价指标为NuScenes 3D指标，这是一种用于评估自动驾驶场景中的3D目标检测方法的综合指标。主要有mAP、NDS、mATE、mASE、mAOE、mAVE和mAAE七个指标。

#text(weight: "bold")[AP(Average Precision)]。  AP值的获取涉及PR曲线，PR曲线计算的是在某一分类阈值下查准率与召回率的曲线与坐标轴形成图形的面积。其中查准率与召回率的计算涉及混淆矩阵，TP（真正类）、FP（假正类）、FN（假负类）、TN（真负类），其中计算公式如下：
$
  "Precosion" = "TP" / ("TP" + "FP") \
  "Recall" = "TP" / ("TP" + "FN")
$
PR曲线的绘制是按照置信度排序后取前$n$个目标计算出的$P$（查准率）与$R$（召回率）坐标，当$n$从$1$取到$N$后便形成一条曲线。PR曲线较为客观的反应了分类方法的性能，而不同情况下算法的分类阈值不同，将不同阈值下的PR曲线面积求平均便得到了AP指数。在NuScenes中，由于目标分类不同，将所有类别的AP求平均即为mAP指数。

ATE（Average Translation Error），平均平移误差，即预测3D坐标中心点与真实框的中心点之间2D欧氏距离；ASE（Average Scale Error），平均尺度误差，即预测3D框的长宽高与真实框的差距，使用3DIoU指标；AOE（Average Orientation Error），平均方向误差，即预测3D框与真实框的偏航角差距； AVE （Average Velocity Error），平均速度误差，即预测的速度3维向量与真实值的差的L2范数；AAE（Average Attribute Error），平均属性误差，定义为1-准确率。

NDS指标为综合指标，为以上6个指标的加权平均值，公式如下，mTP代表上面五个指标的集合：
$
 "NDS" = 1/10 [5 "mAP" + Sigma_("mTP" in "TP")(1 - min(1, "mTP"))] 
$


== 本章小结

本章阐释了本文用到的相关理论和技术。本章首先说明了3D目标检测的数据集的选择和UAV3D数据集的格式。之后详细介绍了Transformer模型，从注意力机制开始深入讲解了Transformer的各个结构以及其作用。在此基础上，本章还引入了端到端的3D目标检测模型DETR3D，并介绍了3D目标检测任务中广泛使用的评价指标NuScenes 3D指标。



#{
  pagebreak()
}

= 时空特征结合端到端3D目标检测算法

== 引言

针对无人机的3D目标检测，上一章完成了面向无人机的3D目标检测数据集UAV3D、3D目标检测任务的详细介绍、Transformer以及DETR3D模型的介绍。本章将详细分析DETR3D和其他3D目标检测算法应用于UAV3D数据集的实验结果，主要基于DETR3D算法的实验结果提出进一步的改进方案。根据DETR3D的实验结果，提出融合数据集的时序信息，加入前一帧模型的预测信息，加速模型收敛，并且增强模型检测的鲁棒性。其次利用数据集中无人机的位姿信息，对比前后帧的位姿可以得到上一帧目标的到下一帧的位置变换矩阵，将从上一帧获取的目标Query经过旋转变换后得到当前帧的相对3D位置。最后在UAV3D数据集上对方法进行了验证。


== 现有算法在UAV3D数据集的检测效果<3.2>

为了测试现有面向车端的多视角3D目标检测模型在UAV3D数据集上的表现，本文选取了BEVFusion、DETR3D和PETR三个经典的多视角3D目标检测模型，分别使用UAV3D数据集进行训练。

训练使用的环境配置为Python 3.8.20，使用RTX3090\*4 和V100\*4 GPU以及Intel(R) Xeon(R) Gold 6226\@2.90GHz 16核\*2。训练数据集UAV3D中的1000个场景（Scene）中70%作为训练集，15%作为验证集，剩下的15%作为测试集，进行训练。

实验结果如下表：

#figure(
    table(
      columns: (auto,auto,auto,auto,auto,auto,auto,auto),
      // rows: auto,
      align: center,
      table.header([模型],[特征提取骨干网络],[图片大小],[*mAP$arrow.t$*],[*NDS$arrow.t$*],[*mATE$arrow.b$*],[*mASE$arrow.b$*],[*mAOE$arrow.b$*]),
      [BEVFusion],  [Res-101],[$800 times 450$],[0.536],[0.582],[0.521],[0.*154*],[0.343],
      [PETR],       [Res-50], [$800 times 450$],[0.581],[0.632],[0.625],[0.160],[*0.064*],
      [DETR3D],     [Res-101],[$800 times 450$],[*0.610*],[*0.671*],[*0.494*],[0.158],[0.070],
      
    ),
    caption: [BEVFusion、PETR、DETR3D模型在UAV3D数据集上的表现]
)<prev_model_fig>

=== 实验结果分析

如@prev_model_fig 中数据所示，综合表现最佳的模型为DETR3D，虽然其mASE略低于BEVFusion而mAOE低于PETR，但都在同一水平，相差不大。而DETR3D的mAP和NDS指标明显优于PETR与BEVFusion模型。

PETR与BEVFusion模型在@1.2.5 中有关于总体的介绍。其中BEVFusion模型支持多模态输入，在NuScenes数据集上的相机与激光雷达多模态输入的mAP指标为68.52%，而多视角相机的得分仅为35.56%。UAV3D数据集面向多视角的相机，所以BEVFusion的表现差于PETR和DETR3D。PETR模型综合表现上略差于DETR3D模型，主要原因在于PETR使用的特征提取骨干网络与DETR3D不同，提取的图像特征不够精细与鲁棒，影响后续结构的训练。

接下来我们将详细分析DETR3D模型的预测结果，并提出改进的思路。

#{
  figure(
    grid(
    columns: 2,
    gutter: 3pt,  // 图片间距
    image("images/Chapter3/RIGHT_gt.png", width: 100%),
    image("images/Chapter3/RIGHT_pred.png", width: 100%),
    ),
  caption: [真实值（左）与DETR3D预测值（右）的对比]
  )
} <detr3d_pred_gt_fig>

@detr3d_pred_gt_fig 是选取的一张右侧摄像头的图片，左边是真实值，右边是DETR3D预测的3D边界框。由于DETR3D使用物体序列来作为物体的位置，其默认个数达到900个，所以右侧图会有较多的3D边界框，可以代表当前图片中DETR3D模型关注的区域。

对比左右图，可以发现DETR3D目前存在的缺点如下：

1、*部分遮挡物体检测失败。* 可以看到，对于部分遮挡的车辆，DETR3D并没有较好的识别，这说明DETR3D模型的鲁棒性不足。


2、*非目标物体分类错误。* DETR3D的Query目标集中在真实值的附近，有时候也会在无车的道路上识别出车辆，尽管模型给出该边界框的置信度较低，但也说明模型没能很好的区分目标与非目标物体。该问题的根源在于第三点，由于数据集中目标时常出现在建筑之后，这一部分目标的图像特征往往呈现为建筑的图像特征，导致模型难以区分。


3、*遮挡物体预测错误。* 第三点的问题根源与第二点相同，数据集中被建筑物或者树木等环境遮挡的目标车辆难以准确预测。完全遮挡物体的预测本身是一个非常有挑战性的任务，这需要模型对于目标的运动轨迹拥有良好的建模能力，并且模拟物体与环境的交互方式以推测遮挡目标的位置信息。


#{
  figure(
    grid(
    columns: 4,
    rows:2,
    gutter: 3pt,  // 图片间距
    column-gutter: 40pt,
    image("images/Chapter3/cropped_gt_1.png", width: 4cm , height: 4cm,),
    image("images/Chapter3/cropped_pred_1.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_2.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_2.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_3.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_3.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_4.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_4.png", width: 4cm, height: 4cm,),
    ),
  caption: [局部放大对比]
  )
}

==  时空特征融合的3D目标检测算法

// 在无人机场景下，目标频繁被遮挡且尺寸较小，导致单帧检测模型难以稳定捕捉目标的空间信息。针对这一挑战，本文提出一种基于时序特征融合的3D目标检测算法，其核心在于结合跨帧跟踪查询（Track Query）与运动补偿机制，通过融合历史帧的时空信息提升检测鲁棒性。该算法以DETR3D为基础架构，引入TrackFormer的跟踪查询传递机制与StreamPETR的位姿对齐策略，形成端到端的时序建模框架。算法通过历史帧信息提取、运动补偿对齐和时序感知解码三个关键步骤，实现了对目标动态特性的高效建模。


在上一小节可以总结无人机场景下，目标频繁被环境遮挡并且距离较远、尺寸较小，导致单帧检测模型难以稳定定位目标的空间位置。针对这一挑战，本文提出一种基于时空特征融合的3D目标检测算法，算法的核心在于结合跨帧查询与运动补偿机制，融合历史帧的时空信息来提升检测的鲁棒性。算法以DETR3D为基础，引入TrackFormer@trackformer22 的跟踪查询传递机制与StreamPETR@streampetr23 的位姿对齐策略，形成端到端的时序建模框架。

=== 算法框架与核心思想

在无人机场景中，目标的动态性主要体现于无人机自身的运动导致坐标系偏移和目标自身的位置移动。基于DETR3D的实验表明，仅依赖当前帧特征时，模型对遮挡目标的召回率下降约12.5%。@trackformer22 为此，本文提出通过跨帧Query传递与运动补偿对齐机制，建立目标状态的时空连续性。具体来说，算法从历史帧中筛选高置信度检测结果，提取其Query嵌入作为当前帧的初始状态，并基于无人机位姿信息计算帧间旋转变换矩阵，将历史Query的3D坐标对齐至当前帧坐标系（运动补偿）。这一设计不仅保留了目标的历史轨迹信息，还通过物理对齐减少了无人机运动引入的定位误差。此外，通过Transformer解码器的时序感知机制，对齐后的Query在多视角图像特征中进行迭代修正，最终实现目标位置的精细化预测。

=== 关键技术实现

*跨帧Query传递机制。*
跨帧Query的核心在于实现目标状态的跨帧关联。具体流程分为两步：首先，对上一帧检测结果按置信度排序，取前$K$个置信度最高的Query，记为$Q_(t-1) = \{q_(t-1)^1,q_(t-1)^2,dots,q_(t-1)^k\}$，每个$q_(t-1)^k$为长度为256的张量。随后，将$Q_(t-1)$与可学习的$N$个Query拼接，形成输入序列$Q_t^("init")=Q_(t-1) union Q_t^("new")$。这种设计通过引用历史帧信息保留目标的运动轨迹，同时用新Query检测新增的目标。在Tranformer解码Query的过程中，$Q_(t-1)$通过交叉注意力机制与当前帧的图像特征进行交互，$Q_(t-1)$的张量表征的坐标从历史位置逐步收敛至当前帧的位置，表示为：$ Q_t^l = "CrossAttention"(Q_t^(l-1),F_"image") $其中$F_"image"$为多视角图像特征，逐层修正使得Query的坐标偏差变小。


#figure(
  image("images/Chapter3/时空特征融合框架.svg", width: 14cm),
  caption: [时空特征融合框架]
)

*运动补偿机制。*
无人机的快速运动会导致历史帧坐标与当前帧存在较大的偏差，可以预想：如果将从上一帧获取的Query的位置提前进行修正，能否让模型在此基础上进一步细化目标位置，提升模型性能呢？在UAV3D数据集中，提供了无人机相对全局坐标系的位置，可以获取到上一个时刻到当前时刻目标的相对坐标的旋转变换矩阵和平移变换矩阵。

算法引入StreamPETR的位姿补偿机制，计算上一帧到当前帧的旋转平移矩阵，将上一帧Query的3D坐标进行旋转平移变换，变换后的坐标即为当前帧的历史物体的感兴趣位置。旋转平移矩阵如下:
$ E_(t-1 arrow t)=E_t^(-1) dot E_(t-1) $ 其中$E_(t-1)$和$E_t$分别为前一帧与当前帧到全局坐标系的旋转平移矩阵
以此坐标为起点，Transformer通过层层的解码器对Query的坐标进行修正，缩短了Transformer中Query坐标的收敛时间，能够让Transformer对于目标位置进行精细的检测。

#figure(
  image("images/Chapter3/运动补偿.svg", width: 16cm),
  caption: [运动补偿框架]
)


== 实验结果

实验使用与@3.2 相同的实验环境训练改进的模型。为了验证跨帧Query传递机制和运动补偿机制的有效性，实验按顺序添加两个模块，称为"Ours-Query"(跨帧Query传递机制)和"Ours-Query+"(跨帧Query传递机制+运动补偿)。实验结果在@imporve_model_fig1 中。

#figure(
    table(
      columns: (auto,auto,auto,auto,auto,auto,auto,auto),
      // rows: auto,
      align: center,
      table.header([模型],[特征提取骨干网络],[图片大小],[*mAP$arrow.t$*],[*NDS$arrow.t$*],[*mATE$arrow.b$*],[*mASE$arrow.b$*],[*mAOE$arrow.b$*]),
      [BEVFusion],  [Res-101],[$800 times 450$],[0.536],[0.582],[0.521],[0.*154*],[0.343],
      [PETR],       [Res-50], [$800 times 450$],[0.581],[0.632],[0.625],[0.160],[*0.064*],
      [DETR3D],     [Res-101],[$800 times 450$],[0.610],[0.671],[0.494],[0.158],[0.070],
      [Ours-Query], [Res-101],[$800 times 450$],[0.623],[0.676],[0.491],[0.156],[0.067],
      [*Ours-Qury+*], [Res-101],[$800 times 450$],[*0.626*],[*0.677*],[*0.490*],[0.156],[0.068],
    ),
    caption: [改进模型在UAV3D数据集上的表现]
  )
<imporve_model_fig1>

从@imporve_model_fig1 中可以看到，跨帧Query传递机制模块显著增强了算法的鲁棒性。引入跨帧Query传递机制的改进算法相较于DETR3D全方面都有着明显的提升，尤其是mAP指标，代表算法的检测精度提升。这表明时空特征的融合有效增强了模型对遮挡目标的召回能力。随后引入的运动补偿机制相较于仅有跨帧Query传递机制的算法只有小幅度的提升，可能的原因在于数据集中无人机飞行速度较低且稳定，Transformer的解码器已经能够较好的学习并进行修正由于运动导致的坐标变换。所以运动补偿模块提升的效果并不明显。但总体来说两个模块都对算法有着积极的改进效果。


== 本章小结

本章主要介绍了基于DETR3D算法改进的时空特征结合的端到端3D目标检测算法。首先在UAV3D数据集上验证了现有3D目标检测模型的表现，并且可视化了DETR3D模型的结果。以可视化的结果为基础提出了两个改进模块：跨帧的Query查询机制和运动补偿机制。经过实验后确认两个模块都对现有算法有着积极的改进作用，mAP指数提升了1.6%。


#pagebreak()

= 多尺度融合的时空特征结合的3D目标检测算法

== 引言

在上一章，针对先前算法对于遮挡物体的识别精度低和非目标物体错误识别的问题，本文引入了跨帧Query传递机制和运动补偿机制，增强了模型的鲁棒性，提升1.6%的mAP指数。

对于无人机的目标检测，由于无人机往往飞行在较高的空中，拍摄的目标与相机距离远，在图像上表现为小目标。对于小目标类型，一种有效的方法是使用多尺度融合检测。在本章，算法的改进主要聚焦于多尺度特征的提取与融合，来优化无人机3D目标检测的精度。

多尺度特征融合算法能够利用卷积网络自下而上提取图像特征，随着图像分辨率的降低，特征图的语义信息逐渐增强，但随之位置信息也逐渐变弱。为了提升检测精度，多尺度特征融合算法将不同分别率下的图像特征进行不同方式的融合。优点在于结合了多尺度的图像信息，对于小目标，其在高分辨率特征图中更容易识别，提高了小目标的识别率和识别精度。多尺度特征融合的特征金字塔网络（Feature Pyramid Networkd，FPN）@fpn17  广泛应用于目标检测和计算机视觉算法。多尺度特征融合算法由于需要计算不同尺度下的目标的相关信息，计算量成倍提升。而FPN结合了多尺度特征融合和快速计算的优点，计算量增加较小且显著提升模型的检测精度。

本章针对无人机视角中目标较小的问题，提出引入特征金字塔网络从而提升模型对于小目标的检测精度。该模型使用ResNet-101中的关键层级作为多尺度的特征图，并使用自上而下的特征融合方式。除此外，FPN还有横向连接机制，该机制是其实现位置信息传递的关键。模型融合目标的位置信息和语义特征，提升了小目标的检测精度。

本章在公开的UAV3D数据集上进行实验，使用改进后的算法并与先前的算法进行了对比。

== 理论方法

=== 相关工作

在计算机视觉任务中，目标的尺度差异是影响任务性能的关键因素之一。早期的方法通过构建图像特征金字塔的方式在不同尺度下提取特征，实现了尺度不变性，但是缺点也很明显：首先是多尺度的图像特征提取导致计算量成几何倍数增长，大幅减慢了推理速度；其次是训练时的内存限制导致无法端到端优化图像特征金字塔，导致训练与测试阶段的特征不同；最后，深层网络限制了低分辨率下小目标特征的提取，导致网络对于小目标难以检测。

许多研究者针对小目标进行了优化。W. Liu等人提出了SSD@ssd17，其通过卷积网络的不同层级直接构建特征金字塔，但仅对高层进行采样，丢失了底层的高分辨率信息。FPN在此基础上进行创新，将高层特征中的语义信息逐步向下传递，并且与横向的连接融合不同层级的特征信息，从而构建具有统一语义表示的金字塔结构。这种方法既能结合图像金字塔的多尺度信息，又避免了多尺度特征提取的计算冗余。

#figure(
  grid(
    rows: (auto,auto),
    columns: (auto,auto),
    gutter: 4pt,
    image("images/Chapter4/1a.png")+text([a.图像特征金字塔]),image("images/Chapter4/1d.png")+text([b.FPN]),
    image("images/Chapter4/1c.png",height: 3.7cm)+text([c.SSD]),image("images/Chapter4/1b.png",height: 3.6cm)+text([d.卷积网络])
  ),

  caption: [各种金字塔结构]
)

=== FPN理论基础

FPN的网络结构主体可以分为三个部分：

*自下而上的特征提取。*FPN没有单独设计网络逐层提取图像特征，而是借用其他的卷积神经网络，卷积网络的前项传播过程天然是金字塔型的特征提取过程。在前向过程中，特征图在经过某些层时大小并不会改变，在某些层则会改变。特征图大小不变的层统称为一个阶段，当特征图大小改变时进入下一个阶段。以ResNet为例，FPN选取的就是C2、C3、C4、C4四个卷积块作为四个阶段，其中C1到C2并没有改变特征的图的大小，因此没有选取。

*自上而下的特征融合和横向连接。*由于自上而下的特征融合和横向连接同时作用于特征图，此处合在一起介绍。通过自下而上的特征提取，FPN得到了4张不同尺度的特征图。在自上而下的特征融合中，FPN使用上采样（Upsampling）方法，是卷积的反向过程。通过上采样，FPN将高层的语义信息逐步融入下层，并且同时横向连接使用$1 times 1$的卷积将同一层的特征图进行处理后与上采样的结果相加，语义信息与高分辨率下的位置信息融合。横向连接使得浅层的细节信息与高层的语义信息有效结合在一起，形成了兼具高分辨率与强语义的特征图。

#figure(
  image("images/Chapter4/fpn.png",height: 8cm),
  caption: [FPN结构示意图]
)

=== 数学建模与优化目标

假设骨干网络生成的特征图为集合$\{C_l\}^5_(l=2)$，FPN生成金字塔特征$\{P_l\}^5_(l=2)$的公式为:
$ P_5 = "Conv"_(1 times 1)(C_5) $
$ P_l = "Conv"_(1 times 1)(C_l) + "Upsample"(P_(l+1)), space space space space l=4,3,2 $
为增强特征表征能力，每个$P_l$后接一个$3 times 3$的卷积层来消除上采样的混叠效应：
$ P'_l = "Conv"_(1 times 1)(P_l) $
在检测阶段，每个金字塔层级$P'l$独立预测对应尺度的目标。FPN的损失函数由分类损失$L_"cls"$和回归损失$L_"reg"$共同组成：
$ L = 1/N_"cls" Sigma_i L_"cls"(p_i,p_i^*) + lambda 1/N_"reg" Sigma_i p^*_i L_"reg"(t_i,t_i^*) $
其中，$p_i$为边界框$i$的预测类别概率，$p_i^*$为真实概率，$lambda$为平衡系数，$t_i$与$t_i^*$为预测边界框与真实边界框的参数化偏移量。

== 实验结果

本章模型在第三章的基础上进行改进，引入FPN网络。其中FPN以ResNet-101的C2、C3、C4、C5层作为金字塔特征的不同阶段。实验环境与@3.2 中相同，实验结果如下。


#figure(
    table(
      columns: (auto,auto,auto,auto,auto,auto,auto,auto),
      // rows: auto,
      align: center,
      table.header([模型],[特征提取骨干网络],[图片大小],[*mAP$arrow.t$*],[*NDS$arrow.t$*],[*mATE$arrow.b$*],[*mASE$arrow.b$*],[*mAOE$arrow.b$*]),
      [BEVFusion],  [Res-101],[$800 times 450$],[0.536],[0.582],[0.521],[0.154],[0.343],
      [PETR],       [Res-50], [$800 times 450$],[0.581],[0.632],[0.625],[0.160],[0.064],
      [DETR3D],     [Res-101],[$800 times 450$],[0.610],[0.671],[0.494],[0.158],[0.070],
      [Ours-Query], [Res-101],[$800 times 450$],[0.623],[0.676],[0.491],[0.156],[0.067],
      [Ours-Qury+], [Res-101],[$800 times 450$],[0.626],[0.677],[0.490],[0.156],[0.068],
      [*Ours-FPN*], [Res-101],[$800 times 450$],[*0.770*],[*0.777*],[*0.270*],[*0.117*],[*0.048*],
    ),
    caption: [基于FPN的改进模型实验结果]
  )<fpn_result>


=== 结果分析

在@fpn_result 中，基于FPN的改进模型在所有指标上取得了非常显著的进步，mAP提升了14.4%，NDS提升了10%，mATE提升了22%等，改进模型所有指标均为最优。由此可见，多尺度特征图对于3D目标检测效果的提升是全方位的，有效地提升了模型对于目标的感知能力。模型结果可视化图如下

#figure(
  grid(
    rows:(auto,auto,auto,auto),
    columns:(auto,auto,auto),
    gutter: 10pt,
    [真实值],[DETR3D],[Ours-FPN],
    image("images/Chapter4/cropped_gt_1.png"),
    image("images/Chapter4/cropped_pred_2_1.png"),
    image("images/Chapter4/cropped_pred_1.png"),
    image("images/Chapter4/cropped_gt_2.png"),
    image("images/Chapter4/cropped_pred_2_2.png"),
    image("images/Chapter4/cropped_pred_2.png"),
    image("images/Chapter4/cropped_gt_3.png"),
    image("images/Chapter4/cropped_pred_2_3.png"),
    image("images/Chapter4/cropped_pred_3.png"),
    image("images/Chapter4/cropped_gt_4.png"),
    image("images/Chapter4/cropped_pred_2_4.png"),
    image("images/Chapter4/cropped_pred_4.png"),
  )
  ,
  caption: [Ours-FPN与DETR3D以及真实值的对比]
)<cmp_fig_2>


通过对比可以很明显看出：1、模型对于目标的识别更加准确。DETR3D经常将非目标物体错误识别为目标，而改进模型几乎没有（@cmp_fig_2 中第二、三行）。对于目标物体，改进模型的识别精度更高，并且少有边界框重叠的情况，而DETR3D经常将边界框重叠在一起（@cmp_fig_2 中第一、二、四行）。2、模型对于遮挡物体的识别与预测更加准确。半遮挡物体与遮挡物体难以检测与预测，改进算法在DETR3D的基础上提升了半遮挡物体的检测和对遮挡物体的预测（@cmp_fig_2 中第三、四行）。

== 本章小结

本章介绍了特征金字塔网络FPN的理论基础和优化函数，并且基于前一章的算法改进，融入FPN。实验结果显示改进算法Ours-FPN相较于第三章的改进算法全面显著增强，mAP指数提升14.4%，NDS指数提升10%，mATE提升了22%。说明多尺度特征提取对于模型的检测能力有着全方面的显著改进。

#pagebreak()

= 总结与展望

== 总结

本文主要介绍和阐述了无人机3D目标检测的意义、研究背景和发展趋势。对目标检测的发展、基于深度学习的3D目标检测算法、Transformer架构、DETR3D模型和算法评测指标进行了总结。本文总结了当前面向无人机的3D目标检测的问题：数据集严重匮乏、数据集制作成本高、模型难以检测遮挡物体、目标识别精度不够等。本文的研究成果主要有以下两点：

一、针对遮挡物体难以检测的问题，提出了一种时空特征融合的改进算法。该算法将上一帧的Query融入下一帧中，称为跨帧Query传递机制。该机制利用了在DETR3D中Query表示目标的位置信息，加快了目标位置的收敛速度，提升了算法的鲁棒性。除此外还引入了运动补偿机制，针对上一帧的目标位置进行修正，再次加速目标位置的收敛速度。实验结果证实改进算法提升了模型的检测精度和鲁棒性。

二、针对高空视角下目标较小的问题，提出引入多尺度特征融合的改进算法。算法引入特征金字塔网络FPN，将ResNet-101的关键层作为FPN的特征图，并使用自上而下的特征融合与横向连接方法生成具有统一表示的高分辨率空间信息与低分辨率语义信息融合的特征表示。实验结果表明改进算法在各方面有着显著的提升，模型的检测精度和鲁棒性大幅提升。

== 展望

随着自动驾驶和无人机技术的不断发展，未来无人机必定在指挥交通领域具有重要意义，对于无人机视角的3D目标检测的需求也会更强烈。未来的研究工作会围绕以下几点开展：

一、探索便捷的低成本数据集制作方式。面向无人机的3D目标检测数据集制作成本高、周期长、工序复杂，这些缺点极大地限制了面向无人机领域的3D目标检测算法的发展。

二、研究针对遮挡的目标检测算法。在无人机位于高空，视角常常被建筑、植物遮挡，如何更好地检测半遮挡和遮挡物体是无人机3D目标检测的关键所在。

三、改进对高度的感知能力。无人机视角是鸟瞰视角，对于高度的感知较弱，对于3D边界框的高度预测精度可能会更低。如何提升无人机对于深度信息的感知是一个难题。