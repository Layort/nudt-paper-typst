#import "@preview/cuti:0.2.1": show-cn-fakebold
#show: show-cn-fakebold
#let tnr = "Times New Roman"
#let fsong = (tnr, "FangSong")
#let song = (tnr, "SimSun")
#let hei = (tnr, "SimHei")
#import "thesis-template.typ": *
#import "templates/i-figured.typ"
#set heading(numbering: "1.")
#show heading: i-figured.reset-counters.with(extra-kinds: ("atom",))
#show figure: i-figured.show-figure.with(extra-prefixes: (atom: "atom:"))
#show math.equation: i-figured.show-equation

#show figure: it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show math.equation: set text(font: "TeX Gyre Termes Math")

#show math.equation.where(block: true): it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show figure.where(kind: table): it => [
  #set figure.caption(position: top)
  #it
]
// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "面向无人机的3D目标检测算法",
  name: "赖宇",
  idnum: "202102001020",
  major2: "无", 
  major1: "人工智能与大数据", 
  college: "计算机学院",
  grade: "2021级", 
  advisor: "邓明堂",
  jobtitle: "副研究员",
  unit: "国防科大计算机学院学员一大队学员五队"
)

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!

= 绪论
== 课题背景及研究的目的和意义

=== 课题背景

无人机（Unmanned Aerial Vehicles）是指无人驾驶的飞行机器，被称为“空中机器人”。近年来，国内外无人机相关技术飞速发展，技术不断成熟，市场规模不断扩大。为了方便侦测与控制，无人机往往会搭载一颗或多颗摄像头，使其拥有极佳的视野。加之其卓越的机动性，无人机被广泛部署在交通监控、精准农业、灾害管理、工业生产和野生动物监控等领域。

2018年9月份，世界海关组织协调制度委员会（HSC）第62次会议决定，将无人机归类为“会飞的照相机”@droneglobal18，可见无人机在视觉方面的重要作用。相比传统路侧端的监控摄像头，无人机的效率与适应性都更强，对推于进计算机视觉（Computer Vision, CV）的相关应用至关重要。

计算机视觉领域是人工智能（Artificial Intelligence, AI）领域的重要研究方向和分支。在最近十年间，计算机视觉领域也得到了飞速发展。计算机视觉是使用计算设备对生物视觉的一种模拟，其依靠摄像头与传感器捕获图像信息，在计算机分析处理后，让计算机能和人类一样理解图像信息，比如：目标检测、图像识别、人体分析等等。计算机视觉因此也应用于生活方方面面，如人脸识别、交通监控、光学字符识别和工厂生产监控。

在计算机视觉领域中，最基础且最重要的问题之一就是目标检测(Object Detection)。其主要任务是在数字图像中检测不同类别的视觉物体的实例，包括感知实例、定位实例和分类实例。目标检测的目的在于提供计算机视觉应用中最基础的信息：物体在哪里？而目标检测中最重要的两个指标是准确率（包括分类准确率和定位准确率）和速度。目标识别也是其他计算机视觉任务的基础，如实例分割(Instance Segmentation)、图像字幕(Image Captioning)、目标追踪(Object Tracking)等。目标检测广泛应用于生活中，如自动驾驶技术、机器人视觉、视频监控、‌工业检测‌、无人安防、智能医学等。

按照输出形式的不同，目标检测可以分为2D目标检测和3D目标检测。目标检测发展的前期主要是2D目标检测算法，如AlexNet、YOLO、RCNN等模型。2D目标检测模型能够在图像上标注物体的位置与类别，例如识别出图片中的人体与动物等，可以检测实例的出现以及其在图像上的2D位置。然而由于RGB摄像头缺少对于深度信息的捕捉，无法将识别的物体对应到真实世界中，导致其难满足真实世界3D空间的需求。而3D目标识别算法利用传感器的数据来估计一系列详细的3D信息，如物体的3D大小、坐标、速度以及朝向等，极大地方便了与真实世界交互的需求。因此，3D目标识别在工业生产和学术研究中越来越重要。


=== 研究目的和意义

近些年随着无人机技术的飞速发展，无人机已被用于环境监测、行人交通监控、灾害疏散和工业生产等多个领域。@dronesreview19 这些应用都依赖于无人机系统上的计算机视觉模块来完成，而检测一个或多个相关的物体是计算机视觉的基础之一。因此，目标识别成为了几乎所有无人机系统的重要任务。

然而，现有的无人机应用的目标识别模型主要是针对传统的2D感知任务设计的，例如LAM-YOLO@LAM-YOLO24 和Drone-TOOD@Drone-TOOD24。2D图片只能提供2D平面上物体的数量和类别等信息，这限制了需要对环境进行3D理解的实际应用的发展。在基于视觉的机器系统中，3D感知扮演着重要的角色，其能够处理2D感知无法胜任的复杂任务。在无人机的相关应用中，鲁棒的目标识别对于无人机的有效部署至关重要。虽然对于无人机来说，3D视觉仍然是相对较新的技术，但它提供了在3D环境中捕获对象的完整维度数据的能力，使无人机在复杂极端情况下也能保持较高的鲁棒性。除了鲁棒性，3D信息的提供也让无人机的能力更加强大。无人机可以更加直观地获取到物体的相对3D坐标，配合无人机自身的坐标，可以快速、便捷地获取物体在真实世界上的3D位置，极大地强化了无人机的监控能力。

随着自动驾驶技术的发展，研究人员开始关注3D视觉下多角度的目标检测技术，出现了DERE3D@detr3d21、PETR@PETR22、PETRv2@PETRv2、BEVFormer@BEVFormer22 等车载3D多视角目标检测模型。这些模型能够通过车身的多个摄像头检测出车辆、行人等物体的3D信息，实时监控周边的交通情况，为自动驾驶提供了重要的3D基础信息。然而，目前尚未出现专注于无人机的3D多视角目标检测模型。同时，针对无人机3D多视角目标检测任务的数据集UAV3D@UAV3D24 日前发布，经过测试，先前的3D多视角目标检测模型在该数据集上表现均不理想，可能的原因是车载摄像头视角下车辆的与无人机视角下的车辆距离相差较大，模型难以成功识别远距离物体对象。这说明先前的研究结果并不能直接应用于无人机领域。如何研发适用于无人机端的3D目标检测模型对于交通管理、工业生产等领域都有着重要的意义和研究价值。

因此，本文将围绕无人机领域的3D目标检测技术深入研究，聚焦于提升无人机对于远距离物体的识别精度，旨在提升无人机对地面的感知能力，促进工业界的视觉产品发展。综上所述，本文的研究不仅具有较高的理论学术价值，同时具有广泛的实际工程应用前景。

== 国内外研究现状及发展趋势

本项目的研究目标位面向无人机的3D目标检测算法。我们将主要介绍无人机、目标检测算法、3D目标检测等几个方面的相关工作。




=== 无人机

无人机因价格便宜、使用方便、对人员安全以及操作人员培训简单而越来越受欢迎。@onmonitoring18 这些优势，加上其的分辨率和强大的跟踪特性，促使它们在各种环境中的使用越来越多。无人机已被用于环境监测，包括空气污染、地表温度、洪水危险、森林火灾、道路表面损坏、地形监测、行人交通监测和灾害疏散。@dronesreview19 例如，许多人因为可以通过移动设备控制的先进产品而提高了生活水平。汽车技术通过提供关于交通的最新和精确信息来帮助驾驶员。无人机有多种规格、尺寸和配置。它们被归类为四大类别：固定翼、混合固定翼、单旋翼和多旋翼，同时考虑旋翼的数量。固定翼无人机适合于航空测量和绘图，因为它们稳定且续航时间长。混合固定翼无人机结合了自动化和手动滑翔，提供了可操作性和效率之间的平衡。单旋翼无人机虽然更复杂且成本更高，但为特定任务（如详细的地形测量）提供了卓越的精确度。最后，多旋翼无人机，尤其是四旋翼无人机，因其敏捷性、垂直起降能力和常用于监控和航空摄影应用而受到高度重视。多旋翼无人机可以是三旋翼、四旋翼、六旋翼或八旋翼。@aidrones24



=== 目标检测算法
// 这里用的 Object Detection in 20 Years: A Survey

// 目标检测对于人类甚至是普通生物都是较为简单的任务，很少会有人无法识别物品。然而在过去几十年间，如何让计算机学会检测各种图片中的示例却是一件很有挑战性的的任务。作为计算机视觉的基础任务之一，目标检测近一直是研究与应用的热点，并在几十年间迅速发展，诞生了许多目标检测算法与模型。依照使用的机器学习算法不同，我们将其分为传统目标检测算法和深度学习目标检测算法。

目标检测作是生物视觉系统与生俱来的核心能力。然而，在计算机视觉领域，自Marr提出视觉计算理论框架以来，实现类生物水平的通用目标检测一直是具有挑战性的任务上。为计算机视觉的基础任务之一，目标检测近一直是研究与应用的热点，并在几十年间迅速发展，诞生了许多目标检测算法与模型。根据特征生成机制的本质差异，现有方法论可划分为两大技术路线：
（1）基于显式特征工程的传统目标检测算法；（2）基于隐式表征学习的深度学习时期的目标检测算法。

=== 传统的目标检测算法

传统的目标检测算法主要兴起于上世纪的九十年代后期，主要使用特征工程和机器学习算法，如支持向量机(Support Vector Machine, SVM)、AdaBoost迭代算法和DPM（DeformablePart Model）以及梯度直方图特征（Histogram of Oriented Gradients,HOG）、局部二值模式（Local Binary Patterns，LBP）等。传统目标检测算法主要可以分为几个阶段：1.图像输入后使用滑动窗口或者选择性搜索来选取候选框；2.对每个框内的图像使用算子提取特征，判断为目标后记录候选框位置；3.使用分类器对候选目标进行识别分类；4.最后对分类识别的结果进行一系列的后处理，比如说非极大值抑制（NMS）来去除多余的候选框，来获取目标的最佳检测的位置。
在传统的目标检测算法中，具有代表性的是Viola Jones检测器@violaJones01，HOG检测器@hog05，基于部件的可变形模型（DPM）@dpm08。

VJ检测器是第一个检测速度能够达到实时检测的人脸检测器。VJ检测器主要由三个关键部件组成：Harr-like特征和积分图、级联分类器、AdaBoost算法。大部分传统目标检测算法需要使用滑动窗口来对图像区域进行检测，然而大量候选区域成为了计算的瓶颈。VJ检测器提出使用强分类器的级联作用快速筛选出不需要检测的区域，从而加快检测速度。而在计算区域特征时，VJ检测器使用Harr-like特征，这是一种卷积运算模板，需要大量的求和计算。因此VJ检测器引入了积分图算法，可以在任意矩形区域内计算像素的和，大大提升了计算速度。然后VJ检测器通过AdaBoost算法训练大量基础分类器，这些弱分类器联合在一起成为一个强分类器。最后将强分类器顺序组合在一起，形成级联结构。如果区域的计算值达不到弱分类器的阈值就会被排除，从而快速筛选区域。VJ检测器检测速度是当时算法的十倍甚至百倍，并且保持着同水平的正确率，成为目标检测算法的里程碑之一。然而，VJ检测器的的鲁棒性不足，很难识别部分遮挡的目标对象。

HOG检测器使用了方向梯度直方图和支持向量机，其中方向梯度直方图（即HOG），将网格的梯度方向信息进行统计，然后使用支持向量机对候选区域进行分类。HOG的这种策略旨在尝试平衡非线性与不变性，加强识别能力的同时又增强泛化性。尽管HOG检测器在行人检测等方面性能表现优秀，其对于光照、复杂场景、旋转位移的处理存在不足。

DPM 吸收了HOG的思想，将基于手工特征的传统目标检测推向顶峰。DPM使用的特征，本质上是HOG特征的改进，取消了HOG特征的块的概念，保留了Cell概念。并使用图像金字塔来采集HOG特征金字塔，从下到上特征从精细变得粗略。DPM使用根滤波器（root filter）和部件滤波器（parts filter）分别匹配整体分数和局部分数，综合得分后成为区域的总体得分。DPM连续三届获取CVPR VOC的冠军，作者也被VOC授予终身成就奖，可见DPM的性能优异。然而，DPM只对刚性物体的检测效果好，并且模型复杂，计算复杂度高，训练过程耗时，模型参数多且固定导致调制过程繁琐困难。

传统目标检测算法高度依赖特征工程，其核心是研究者基于经典的图像处理理论（如边缘检测算子、梯度计算模型和纹理分析算法）构建特征提取范式。这类特征设计必须严格遵循人类可解析的数学公式或物理先验知识（如Canny边缘检测中的梯度阈值设定、HOG特征的方向梯度直方图统计），导致特征表征能力被限制在人类认知框架内，难以捕捉图像中复杂的抽象模式与跨尺度关联特性，最终难以适应开放场景下目标形态的多样性变化。受限于早期算力瓶颈，图像特征也只能构建浅层特征组合。即使是特征工程的巅峰时期，基于专家知识构建的复合特征也仅有$10^3$维，相较于目前深度学习动辄就能抽取$10^6$维以上的图像特征存在数量级上的巨大差异。

=== 深度学习时期的目标检测算法

早在1958年，人工智能专家F.Rosenblatt就提出了感知机模型;@perceptron58 到了1986年，Hinton等提出了基于Sigmoid激活函数的多层感知机模型MLP与反向传播算法;@bp86 然而由于计算水平和数据水平的不足，深度学习未受到重视。直到2012年Hinton和他的学生Alex Krizhevsky设计的AlexNet@alexnet17 在ImageNet竞赛取得了突破性的成果后，深度学习才真正获得了人们的关注。 自此以后，深度学习的发展走上了快车道，各种深度学习算法如雨后春笋般出现，在目标检测领域就有RCNN、Fast-RCNN、FPN、YOLO、DETR等算法。按照算法处理阶段的不同，我们将算法分为基于CNN的两阶段检测器和基于CNN的单阶段检测器。

A. 基于CNN的两阶段检测器。

2014年，R.Girshick等人首次将深度学习技术运用在目标检测领域，提出了带有CNN特征的区域（Region-CNN，R-CNN）@rcnn14，R-CNN也是首个两阶段检测器。R-CNN的idea较为朴素：第一步通过选择性搜索策略提取一组潜在候选区域。然后将每个候选区域缩放为固定大小的图像，并输入预训练的CNN 模型（如 AlexNet）中以提取特征。最后使用SVM分类器预测每个区域内是否存在对象并识别对象类别。R-CNN在VOC07数据集@pascal-voc-2007 上有显著的性能提升，平均精度(mAP)从33.7%大幅提升至58.5%。R-CNN作为首个使用深度学习的目标检测算法，很难做到完美，最大的缺点就是庞大的计算量：对大量重叠的候选区域进行冗余特征计算导致检测速度极慢，在使用GPU的情况下每张图像需要14秒。

同年，何凯明等人提出了空间金字塔池化网络(SPPNet)@sppnet15，以解决R-CNN速度慢的问题。以前的CNN模型仅能处理固定大小的输入，例如AlexNet只能输入大小为224x224的图像。SPPNet的主要贡献是引入了空间金字塔池化 (SPP) 层，这使得不管图像的大小如何，模型都能够生成固定长度的表示，无需重新缩放。使用SPPNet进行目标检测时，计算一次特征图便可以生成任意区域的固定长度表示，从而避免重复计算卷积特征。SPPNet比R-CNN快20倍以上，且没有牺牲检测精度，在其VOC07数据集上的mAP达到了更高的59.2%。SPPNet仍然存在一些缺点：首先，训练仍是多阶段的，导致过程繁琐复杂；其次，SPPNet简单地忽略了所有前面的层，只对全连接层进行微调，存在着很大的优化空间。

2015年，R.Girshick等人在R-CNN和SPPNet的基础上进一步研究，提出了Fast R-CNN@fastrcnn15。Fast R-CNN能够在相同的网络配置下同时训练检测器和边界框预测器。在VOC07数据集上，Fast R-CNN将mAP从R-CNN的58.5%提高到了70.0%，同时检测速度比R-CNN快200倍以上。虽然Fast-RCNN成功地整合了R-CNN和SPPNet的优势，但仍然有改进空间，其检测速度受到候选区域生成速度的限制。在这之前，候选区域往往是使用滑动窗口生成的，数量庞大且生成速度较慢。意识到缺点后，很自然地能想到：“能用CNN模型生成候选区域吗？”接着，Faster R-CNN应运而生。

2015 年，S. Ren 等人在Fast R-CNN提出不久之后就提出了Faster R-CNN检测器@fasterrcnn17。Faster R-CNN 是第一个检测速度接近实时的深度学习检测器，其在COCO数据集@coco15 上的mAP指标达到了42.7%，VOC07数据集的mAP为73.2%，使用ZF-Net时FPS（Frame per Second）为17。Faster-RCNN 的主要贡献是引入了区域候选网络 (RPN)@fasterrcnn17，从而实现了几乎0成本的候选区域提取。从R-CNN到Faster R-CNN，对象检测系统的大多数单个模块，例如候选区域检测、特征提取、边界框预测等，已逐渐集成到统一的端到端学习框架中。Faster R-CNN突破了Fast R-CNN的速度瓶颈，但在后续的检测阶段仍然存在计算冗余。后来，人们提出了各种改进，包括 RFCN@rfcn16 和 Light head R-CNN@lighthead17。 

2017 年，T.-Y.Lin等人提出了特征金字塔网络FPN@fpn17。在FPN之前，大多数基于深度学习的检测器仅在网络顶层的特征图上进行目标检测。深层次的图像特征蕴含了较多类别信息，却难以提取位置信息。为此，FPN使用了一种具有横向连接的自上而下的架构，能够在所有尺度上构建高级语义。由于CNN通过其前向传播自然形成特征金字塔，FPN在检测各种尺度的物体方面具有显著的优势。在基本的Faster R-CNN模型中使用FPN，它在COCO数据集上实现了当时最先进的单模型检测精度。现在FPN已成为许多最先进的检测器的基本模块。


B. 基于CNN的单阶段检测器。

两阶段检测器遵循经典的级联处理流程。第一阶段注重于生成候选区域，该阶段以提高召回率为核心优化目标；第二阶段基于候选区域提取特征，通过分类与回归网络（Classification & Regression Head）实现细粒度边框定位与判别性特征学习。此类方法凭借两阶段任务解耦的特性，无需复杂的后处理机制就可以实现较高的检测精度。然而，双阶段串行处理的计算范式导致推理速度受限，难以满足工业场景中的低延迟、高吞吐的需求。相比之下，单阶段检测器采用密集锚点或中心点策略，通过单次前向传播便可以直接输出目标的位置与类别。得益于端到端的轻量化设计，单阶段检测器在各种移动设备上具有较大的优势。

在单阶段目标检测算法的发展历程中，YOLO（You Only Look Once）系列模型具有里程碑意义。由Redmon等人于2015年提出的初代YOLO，首次将网格化预测机制（Grid-based Prediction）引入目标检测领域，其核心思想是通过单次前向传播完成图像全域的边界框回归（Bounding Box Regression）与类别概率预测（Class Probability Estimation）。该模型在PASCAL VOC 2007数据集上实现52.7% mAP\@0.5的同时，推理速度达到45-155 FPS，显著超越传统两阶段方法。然而，YOLO的空间分离约束（Spatial Separation Constraint）导致其对密集目标和小物体的定位精度不足，这一问题在后续迭代的SSD与YOLO自身改进版本中得到针对性优化 。2025年2月，YOLO最新版本YOLOv12发布@yolov12 ，这一版引入了Attention机制，使其能够在提升精度的同时保持极高的推理速度（YOLOv12-N的单图像延迟为1.65ms），可见YOLO系列模型的学术生命力之鲜活。

单次多框检测器（Single Shot MultiBox Detector, SSD）由Liu等人于2016年提出，其创新点在于多尺度特征金字塔（Multi-scale Feature Pyramid）与预定义锚框（Predefined Anchors）策略的结合。通过在不同层级特征图上部署不同长宽比的锚框，SSD有效提升了小目标检测的召回率，在COCO数据集上达到46.5% AP\@0.5，快速版本推理速度达59 FPS。相较于YOLO的单一尺度预测，SSD的层级特征融合机制成为后续模型设计的基准范式。

尽管单阶段检测器在速度上占据优势，但其精度长期落后于两阶段方法。Lin等人于2017年通过类别不平衡理论分析（Class Imbalance Analysis）揭示根本原因：训练过程中大量简单负样本主导梯度更新，导致难样本学习不充分。为此提出的RetinaNet引入焦点损失函数（Focal Loss），通过动态调整难易样本的损失权重，使模型在COCO数据集上实现59.1% AP\@0.5，首次达到与两阶段方法相当的精度水平。

基于关键点检测的创新中，Law等人提出的CornerNet摒弃了传统锚框设计，转而通过角点热力图（Corner Heatmaps）与嵌入向量匹配（Embedding Matching）实现边界框生成，在COCO数据集上取得57.8%的AP\@0.5。该方法的无锚点（Anchor-free）特性显著降低了超参数调优难度，但角点分组依赖的后处理步骤仍存在计算冗余。Zhou等人进一步优化的CenterNet将目标建模为中心点热力图（Center Heatmaps），通过中心点直接回归目标尺度与位置，在消除非极大值抑制（NMS）后处理的同时，将精度提升至61.1% AP\@0.5，展现了端到端检测框架的潜力。

Transformer架构的引入标志着目标检测进入全局注意力驱动（Global Attention-driven）的新阶段。Carion等人于2020年提出的DETR首次实现完全端到端检测，通过集合预测（Set Prediction）机制和编码器-解码器架构（Encoder-Decoder Architecture）替代手工设计的锚框与NMS。然而，其平方级计算复杂度（O(N²) Complexity）导致训练收敛缓慢且小目标检测性能受限。Zhu等人提出的Deformable DETR引入多尺度可变形注意力（Multi-scale Deformable Attention），在COCO数据集上以71.9% AP\@0.5 刷新性能记录，同时将训练周期缩短至原算法的1/10 。

=== 基于RGB图像的3D目标检测算法 <1.2.5>
// 这里用的文章是 3D Object Detection for Autonomous Driving: A Survey


2D目标检测一定程度上促进了3D目标检测的发展。3D目标检测方法可以分为单视角和多视角两个方面。
@odsurvey22

单视角3D目标检测方法思想大多源于二维检测框架，通过单目或立体图像直接推理目标三维属性，其主流技术路径可归纳为三类：基于模板匹配的候选框生成、基于几何约束的姿态推导以及基于伪激光雷达的跨模态迁移。

基于模板匹配的方法以穷举采样为核心策略，典型代表如Chen等人提出的3DOP@3dop15，通过立体图像深度估计构建三维点云，将候选框生成转化为马尔可夫随机场（MRF）的能量最小化问题，依赖人工设计势函数（如地平面连续性约束）优化空间分布，最终通过Fast R-CNN@fastrcnn15 实现目标定位；Mono3D@mono3d16 则针对单目相机场景，采用滑动窗口在三维空间采样候选区，假设地平面正交于图像平面以降低搜索复杂度，但需依赖语义分割过滤无效区域，其过度工程化设计导致复杂场景泛化性受限。

基于几何属性的方法摒弃冗余候选生成，转而利用二维检测框的几何特性推导三维姿态：Deep3DBox@3dbox17 强制三维角点透视投影与二维框边缘对齐，通过可微几何约束优化边界框参数；Li等人提出的GS3D@gsd3d19 在Faster R-CNN@fasterrcnn17 框架上引入方向预测分支，联合二维框与粗略三维框的可见面纹理特征进行融合，通过表面特征消除视角歧义，但其依赖经验性几何假设（如目标中心点投影与二维框顶部对齐），在目标尺度突变时引发误差累积。

基于伪激光雷达的方法通过单目深度估计构建伪点云，复用点云检测框架：Xu等人的MF3D@mf3d18 融合RGB图像与视差图生成的前视图特征，通过多级特征拼接增强小目标检测能力；Weng等人提出的Mono3D-PLiDAR@mono3d-plidar19 引入2D-3D边界框一致性损失（BBCL）约束空间对齐，并利用Mask R-CNN@MaskR-CNN20 的实例掩码剔除截锥体外噪声点，但单目深度估计的固有误差（如距离相关的深度伪影）仍限制其远距离检测精度。总体而言，单视角方法虽在实时性与硬件成本上占据优势，但其性能瓶颈仍源于几何先验的强假设与跨模态表征的次优解耦，这驱动研究者向多传感器融合与端到端联合优化方向持续探索。


基于多视角的3D目标检测方法。
// 这里基于https://zhuanlan.zhihu.com/p/686868635

这些方法首先将多幅图像转换成前视图或鸟瞰图(BEV)展示，在网格中密集以利用 CNN 和标准 2D 检测方案。

Wang等人提出了DETR3D@detr3d21。该算法是DETR在三维目标检测领域的延伸，它通过几何反投影和相机变换矩阵将二维特征提取与三维目标预测联系起来，实现了无需密集深度估计的三维目标检测。DETR3D将多视图检测问题转化为集合到集合的预测任务，通过预设的object queries和神经网络解码出三维空间中的参考点，再将这些点反投影到二维特征图上，通过双线性插值采样特征值，最终通过多头注意力机制和Transformer@transformer17 解码器来优化queries并预测边界框和类别。然而DETR3D存在预测参考点不准确、无法从全局角度进行表示学习等缺点，为此，Liu和Wang等人提出了PETR@PETR22。PETR通过引入3D坐标生成器和3D位置编码器，将三维坐标的位置信息编码为图像特征，从而实现多视图三维目标检测。PETR首先在三维空间中初始化一组均匀分布的anchor points，然后通过MLP网络生成初始对象查询。与DETR3D不同，PETR先预设三维坐标再编码到query，这样做避免了在图像平面找不到对应点的问题，并实现了在三维特征空间中的训练。之后Liu等人提出了PETRv2@PETRv2，PETRv2在PETR的基础上增加了时间建模，通过将前一帧的特征与当前帧的特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。PETRv2通过特征引导的位置编码器将图像特征和三维位置信息结合，隐式引入了视觉先验，提高了模型的性能。

Huang等人提出的BEVDet@BEVDet22 是另一种高性能的多相机三维目标检测方法，它在鸟瞰图（BEV）空间中进行目标检测。BEVDet面临过拟合问题，因为它在BEV空间下过度拟合。为了解决这个问题，BEVDet应用了定制的数据增强策略和尺度NMS（Scale-NMS），以提高模型的泛化能力和检测性能。
之后Huang等人提出了BEVDet4D@BEVDet4D22，BEVDet4D通过将前一帧的特征与当前帧中的相应特征融合来捕获时间线索，简化了速度预测，并实现了不同帧目标位置的时间对齐。Li等人提出的BEVFormer@BEVFormer22 利用可变形注意力机制设计了空间交叉注意力和时间自注意力，分别从跨摄像机视图的感兴趣区域提取空间特征和循环融合历史BEV信息，从而实现对三维场景的理解和目标检测。



== 研究内容以及技术路线

本文以无人机为主体，做了什么事情。

最后，达到了什么效果。

技术路线图如下：
#{
set text(size: 10.5pt)
figure(image("images/UAV3D_town10.jpg"), caption: "技术路线图")
}
\

#{
  pagebreak()
}


= 相关基础知识介绍

== 引言

在前文中已经介绍了目标检测，其作为计算机视觉的基础任务之一，拥有重要的研究价值。然而，由于计算资源和传感器的发展受限，目标检测的早期研究注重于2D目标检测任务。2D目标检测的任务难度较低，数据集制作也更为方便。2D目标检测数据集仅需要单目相机，目标标注也只涉及图像上的2D信息。
与之相比，精确的3D目标检测需要获取对象相较于传感器的3D位置信息，对于算法的空间建模能力要求高，是人工智能领域的难题之一。尤其困难的是数据集的收集与标注，不仅需要使用多个或多种传感器来收集3D信息，还需要对目标对象进行精确的3维坐标标注，导致3D目标检测数据集制作成本高昂。

不过随着增强现实(Augmented Reality,AR)、自动驾驶和其他机器人导航系统等技术的应用也推动3D目标检测快速发展。在这些应用中，如自动驾驶，需要算法彻底了解周围环境，不仅是物体的种类，还要求准确获取其姿态与朝向，来规划路线以避免碰撞，这正是3D目标检测擅长的。

随着车端开始从2D目标检测发展3D目标检测，作为无人系统的无人机也需要将传统的2D目标检测转向3D目标检测。但目前专门适用于无人机的3D目标检测数据集极其稀少，并且同时也鲜有人研究面向无人机的3D目标检测算法。接下来，我们将首先介绍无人机视角的3D目标检测数据集的选取与分析；然后我们将深入探究Transformer的基本原理，并且介绍端到端的3D目标检测模型DETR3D；最后我们介绍3D目标检测的性能评价指标。



== 数据集选取与分析

无人机目标检测数据集有很多，包含许多的目标对象，如VisDrone 数据集、UAVDT 数据集、ITCVD数据集、UCAS-AOD数据集等等，然而这些数据集均为2D目标检测任务，无法使用。比较著名的3D目标检测数据集有KITTI数据集、Waymo Open 数据集、NuScenes数据集、Appllo Scape数据集等数据集。不过这些数据集面向自动驾驶任务，采集的数据均为车端数据。目前，仅有UAV3D数据集为唯一开源的无人机3D目标检测数据集。



#{
set text(size: 10.5pt)
figure(image("images/Chapter2/KITTI.png")+ image("images/Chapter2/NuScenes.png"), caption: "KITTI数据集（上）和NuScenes数据集（下）")
}

=== 数据收集部分

由于在高空中目标的3D坐标以及其他信息难以测算和校准，UAV3D数据集使用CARLA模拟器@dosovitskiy2017carla 来模拟车流环境并记录数据，并使用AirSim模拟器@shah2018airsim 来模拟无人机飞行。在CARLA中，车辆生成后会随机导航到穿过城镇。在数据集中一共有3、6、7和10号4个城镇，这四个城镇都有着复杂的交通情况，有着许多红绿灯路口和T型路口。在每个城镇记录250个场景（Scene），总共1000个场景。

为了充分模拟复杂的飞行场景，UAV3D数据集4个城镇分为Urban（3,10号城镇）与SubUrban（6,7号城镇）。在CARLA模拟器中，10号城镇因为交通场景复杂且拥塞而出名，6、7号城镇车辆则相对稀少，这种城镇与郊区的均衡还体现在行人、建筑、车辆与道路标记上。对于每个城镇，无人机都有25条线路来尽可能地覆盖整个城镇区域。

UAV3D数据集面向3D目标检测任务，其在无人机上设置了不止一个摄像头。在无人机的前后左右和底部分别有一个摄像头，以保证无人机有足够的感知范围。四周的摄像头倾角为-45°，底部的摄像头则为水平放置，提供鸟瞰（BEV）图像。每个摄像头拍摄图像的分辨率为800×450像素。CARLA模拟器使用UE坐标系（即左手坐标系），x轴朝前，y轴向右，z轴朝上。但AirSim使用NED坐标系（North East Down），NED坐标系是导航领域常用的坐标系，三个轴分别指向地球北极、东方向和地心。在UAV3D中，作者将传感器的坐标从AirSim坐标转换为UE坐标系，以方便后续的对齐工作。

多无人机协同也是无人机的研究热点之一，UAV3D数据集为此添加了无人机纵队，一共五台无人机，位置分别为前后左右以及中间，四周的无人机与中间无人机的距离为20米。整个无人机纵队在60米的高中进行感知和协同任务。




#{
set text(size: 10.5pt)
grid(
  columns: 2,
  column-gutter: 2em,
  row-gutter: 1em,
  figure(
    image("images\Chapter2\UAV3d_uav_structure.png", height: 4.82cm),
    caption: [传感器于无人机上的分布]
  ),
  figure(
    image("images\Chapter2\UAV3d_uav_cross.png",  height: 4.82cm),
    caption: [无人机纵队示意]
  )
)
}


=== 数据标注及与其他数据集的对比

UAV3D将5架无人机的5个摄像头同时记录下来作为一个样本（sample），即一个样本有25张图像。UAV3D从CARLA模拟器中获取车辆的3D信息作为标注，从AirSim模拟器中获取相机的内存与外参矩阵提供给下游任务。UAV3D的标注信息包括目标的3D边界框以及像素级别的语义分割标签。每个3D边界框包含物体中心坐标（x,y,z）、边界框的长宽高以及物体的朝向角（yaw,pitch,roll）。前文提到UAV3D有1000个Scene，每个Scene包含20个sample，总共有50万张图片和330万个3D边界框。这些数据被分为训练、验证与测试三个部分，格式与使用较多的nuScenes数据集相似，可以直接使用nuScenes-devkit工具库进行读取加载。

下表为UAV3D数据集与其他数据集的对比。V2X、V2V和V2I分别表示设备对万物、设备对设备和设备对基础设施的合作，C、 L和R分别指摄像头、激光雷达和雷达传感器。

#{
figure(
table(
  columns: (58pt,auto,auto,42pt,auto,auto,auto,auto,auto,auto,auto),
  rows: (15pt, 18pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt,15pt), 
  align: center,
  // inset: 5pt,
  table.header(
    [数据集], [年份], [来源], text(size: 7.8pt)[应用场景], [V2X],[模态],[场景], [帧数], [图数], text(size: 9pt)[标注数], [类别]
  ),
  [VisDrone @zhu2018vision], [2018], [真实], [无人机], [无],[C] ,[–], [18万], [1.02万], [无], [10],
  [UAVDT @du2018unmanned], [2018], [真实], [无人机], [无],[C],[–], [8万], [8万], [无], [3],
  [Waymo @sun2020scalability], [2019], [真实], [驾驶],[无],[C&L],[1000], [20万], [100万], [120万], [4],
  [nuScenes @caesar2020nuscenes], [2019], [真实], [驾驶], [无],[C&L&R],[1000], [4万], [140万], [140万], [23],
  [OPV2V @xu2022opv2v], [2022], [模拟], [驾驶], [V2V],[C&L],[–], [–], [4.4万], [23万], [1],
  text(size: 9.81pt)[V2X-Sim @li2022v2x], [2022], [模拟], [驾驶], [V2X],[C&L],[–], [–], [6万], [2.66万], [1],
  [V2XSet  @xu2022v2x], [2022], [模拟], [驾驶], [V2X],[C&L],[–], [–], [4.4万], [23万], [1],
  text(size: 8.2pt)[DAIR-V2X @yu2022dair], [2022], [真实], [驾驶], [V2I],[C&L],[–], [–], [3.9万], [46.4万], [10],
  text(size: 5.4pt)[CoPerception-UAV @hu2022where2comm], [2022], [模拟], [无人机], [V2V],[C],[183], [0.44万], [13.2万], [160万], [21],
  [V2V4Real @xu2023v2v4real], [2023], [真实], [驾驶], [V2V],[C&L],[–], [–], [4万], [24万], [5],
  [Rcooper @hao2024rcooper], [2024], [真实], [驾驶], [V2I],[C&L],[–], [–], [5万], [–], [10],
  text(size: 6.68pt)[TUMTraf-V2X  @zimmer2024tumtraf], [2024], [真实], [驾驶], [V2I],[C&L],[–], [–], [0.5万], [2.93万], [8],
  [HoloVIC @ma2024holovic], [2024], [真实], [驾驶], [V2I],[C&L],[–], [10万], [–], text(size: 9.8pt)[1140万], [3],
  text(size: 9.3pt)[V2X-Real @xiang2024v2x], [2024], [真实], [驾驶], [V2X],[C&L],[–], [–], [17.1万], [120万], [10],
  text(weight: "bold")[UAV3D], [2024], [模拟], [无人机], [V2V],[C],[100], [2万], [50万], [330万], [17]
),
caption: [UAV3D与其他数据集 ],
)

}

== Transformer基本原理

在Transformer之前，许多先进模型的都依赖于RNN。尽管出现了LSTM、GRU等门控RNN，RNN对于长序列的记忆能力仍然不足，句末的单元往往缺少前端标记的精确信息。同时，RNN的每个单元都依赖于序列靠前单元的结果，导致RNN无法并行处理，训练效果底下。随着谷歌团队在《Attention is All You Need》论文中提出注意力机制（Attention Mechanism），这些问题都得到了解决。而Transformer结构也证明注意机制足够强大，Transformer也逐渐成为了现今最流行的深度学习模型。

=== 整体结构

Transformer的整体结构主要分为左右两端（如@transformer_fig），左边为编码器（Encoder），右边为解码器（Decoder）。一层Transformer由一个编码器和解码器组成，而Transformer可以堆叠多层以提升模型的能力，而模型的训练难度也随之成倍增加。在《Attention is All You Need》原文中，作者权衡训练成本与效果最终使用了六层结构的Transformer。

输入的序列在经过Embedding后进入编码器，并将处理后的序列传递给下一层的编码器以及同层的解码器。同层的解码器将输入的某个特定序列和编码器的输出一起处理并传递到下一层的解码器。直到最后一层的解码器得到输出并使用其他的神经网络如MLP来得到最终的结果。

接下来我们将从注意力机制开始逐步介绍Transformer。

#{
  figure(
    image("images\Chapter2\Transformer_chinese.svg",height:10cm),
    caption: [Transfomer 结构]
  )
}<transformer_fig>





=== 多头注意力机制 <attention>

注意力机制如其名，灵感来源于人类的注意力。大脑在处理信息时，往往会集中关注某些区域，这些区域往往是信息中关键的部分，过滤掉无用的信息，从而提升信息的处理速度和精度。这种信息处理策略被称为注意力机制。以此为启发，研究员提出了神经网络的注意力机制，广泛运用于各种深度学习模型中，取得了巨大的成功。本小节我们主要介绍Transformer中的自注意力机制（self-attention）。

自注意力机制是一种特殊的注意力机制，对于序列本身进行注意力计算，给不同的元素分配不同的权重以获取序列内部的联系。自注意力机制的核心思想是学习映射来查询向量Q（Query）、K（Key）、V（Value）之间的权重关联，进而构建全局的的关联权重。每个序列中的单元与该序列中的所有单元进行注意力计算，自适应地学习到输入序列中的关键信息，并自动捕获不同子空间间的相互关系。

#{
figure(
  image("images\Chapter2\QKV.png",height: 4cm),
  caption: [Self-Attention机制的本质思想]
)
}
#let attention = math.op("Attention")
#let softmax = math.op("softmax")

在Transformer中，输入的序列会在线性映射后加入预设好的位置编码，形成自注意的输入序列$x$。通过三个不同的转换矩阵$W_q$，$W_k$,$W_v$转换为不同的输入序列：Query Token、Key Token、Value Token，即为Q、K、V。自注意机制使用缩放点积注意力（scaled dot-product attention），Query查询Key后得到一个注意力权重$Alpha$,其中$alpha_(i,j)$为$q_i$和$k_j$的点乘。之后将$a_(i,j)$除以$sqrt(d_k)$来增强训练的鲁棒性，并使用softmax函数来将权重归一化。这一系列计算可以使用矩阵来表示，即

$ attention(Q,K,V) = softmax((Q  K^T )/sqrt(d_k) ) V $

其中 $ Q = X dot W_q ; K = X dot W_k ; V = X dot W_v $
矩阵化极大的加速了自注意力机制的运算，使其有着极大的加速空间。


为了增强自注意机制对于多方面信息的思考能力，让其对于逻辑语义能够有更加细腻全方面的思考，Transformer中加入了多头自注意力机制。我们可以将自注意力机制中的$W_q,W_k,W_v$矩阵看做一个读取头，其中的参数是模型对于输入$X$的一种理解。如果我们加入$n$个$W_i^q,W_i^k,W_i^v$矩阵，这样模型便拥有了$n$个头，每个头都能够对$X$产生某个方向的理解。最后多头注意力机制将所有头产生的结果综合，生成最后的结果。公式表达如下
#let multihead =  math.op("MultiHeadAttention")
#let concat = math.op("Concat")
$
 multihead(Q,K,V) = concat("head"_1, dots,"head"_h)W^O\
 "where" "head"_i = attention(X W^Q_i,X W^k_i,X W^V_i) \
  \( W_i^Q in bb(R)^(d_(m o d e l) times d_k) , W_i^K in bb(R)^(d_(m o d e l) times d_k) , W_i^V in bb(R)^(d_(m o d e l) times d_v) , W_i^O in bb(R)^(h d_v times d_(m o d e l)) \) 
$



=== 编码器

Transformer使用了Seq2Seq（sequence to sequence，序列到序列）的经典架构：编码器-解码器（Encoder-Decoder）。编码器的主要作用是将输入的序列编码成同样长度的序列，交由解码器处理。
#{
  figure(
    image("images\Chapter2\EncoderChinese.svg",height:8cm),
    caption: [Encoder 结构]
  )
}
Encoder首先对原始序列$X$进行“预处理”：首先使用词嵌入（Word Embedding）的方式对X进行编码，将X中的每个Token转换为对应的向量，这种方式可以更好的表征Token；然后引入位置编码（Positional Encoding），自注意机制没有采用RNN结构，使用全局信息，但弊端是无法利用Token的顺序信息。位置编码的引入使得自注意力机制能够利用Token的顺序信息，其计算公式如下
#let PE = math.op("PE")
$ 
  PE("pos",2 i) = sin("pos /"10000^(2 i"/"d)) \ 
  PE("pos",2 i+1) = cos("pos /"10000^(2 i"/"d))
$

处理好后的序列X进入多头自注意力模块，其机制在上一小节介绍过。多头自注意力模块的输出后经过Add & Norm层。其中计算公式如下
#let LayerNorm = math.op("LayerNorm")
$ X = LayerNorm(X + multihead(X)) $ 
其中Add代表X与多头注意力模块的结果相加，本质上是一种残差网络。残差网络可以有效解决深层网络训练时的梯度消失和梯度爆炸问题，而Norm是一种正则化，为层正则化（Layer Normalization），能够将每一层神经元的输入转成相同的均值方差，可以加速训练时loss的收敛。

经过多头注意力模块和Add & Norm层后是前馈层（Feed Forward），结构较为简单，为两层的全连接层，第一层使用ReLU激活函数，第二层无激活函数，计算公式如下：
$ X = max(0,X W_1, + b_1)W_2 + B_2 $
之后再是一层Add & Norm层，这样便组成了一个编码器块（Encoder Block）。



=== 解码器与损失计算

解码器将编码后的序列$X$进行解码，得到最终的输出序列。Transformer的解码器是一个自回归解码器，其与编码器的主要区别为中间加入了额外一层编码器-解码器注意力(Encoder-Decoder Attention，也叫Cross Attention)，即编码器与解码器的连接点，。

#{
  figure(
    image("images\Chapter2\DecoderChinese.svg",height:10cm),
    caption: [Decoder 结构]
  )
}

编码器-解码器注意力与普通的多头自注意力模块的不同点在于$K,Q,V$的来源不同。在@attention 中我们详细介绍了多头自注意力机制，其中自注意力机制的$Q,K,V$均来源于输入$X$。而在编码器-解码器注意力中，$Q,K,V$的来源不一致，其中$K,V$来自于编码器的输出，而$Q$则是编码器经过掩码多头自注意力模块得到的。计算公式表示如下
$
  X_("decoder")^' = softmax(W_Q X_("decoder")W_K X_("encoder")) W_V X_"encoder"\
  X_("decoder")^' = LayerNorm(X_("decoder") + X_("decoder")^')
$

值得一提的是解码器的掩码多头自注意力模块（Masked Multihead Attention）。由于解码器预测序列是逐个输出，所以解码器不能使用当前时间步之后的信息，需要在计算的过程中遮盖对应部分。因此编码器引入了掩码操作（Mask），在计算注意力分数时（即$Q dot K^T$）使用掩码矩阵将后序时间步上的Token信息置为空。
#{
  figure(
    image("images\Chapter2\MaskAttention.png",height:3cm),
    caption: [MaskAttention]
  )
}

其余部分与编码器相同，最后解码器的输出经过一层线性层变换后使用Softmax得到了每个Token对应的概率，取最大值即为结果。

不同的下游任务决定了训练Transformer使用的损失函数，在论文原文中任务为预测任务，所以使用的为交叉熵损失（Cross Entropy Loss）。

== 端到端3D目标检测模型DETR3D

DETR3D模型基于DETR模型改进而来，将端到端检测框架带入3D目标检测领域，其通过query来预测目标在3D空间的位置。由于不需要进行深度估计而避免了复合误差的影响，这种自上而下的方法优于自下而上的方法。并且DETR的端到端框架不需要非极大值抑制（NMS）等后处理，极大的提高了推理速度。

=== 端到端目标检测框架 DETR

DETR模型使用了一种全新的视角来看待目标检测任务，是首个将目标检测重构为集合预测任务的端到端模型。其思路是将目标检测看作一个集合预测的问题：预测目标图片中的边界框的集合。DETR利用Transformer模型，将集合预测的任务巧妙转换成seq2seq（sequence to sequence，序列到序列）形式。DETR的核心架构由三部分组成：1、特征提取骨干网络；2、Transformer的编码器-解码器架构；3、集合预测损失函数。我们依次介绍这三个部分。

#figure(
  image("/images/Chapter2/DETR.png",height: 3cm),
  caption: [DETR模型结构图]
)

#text(weight: "bold")[特征提取骨干网络]。将输入的图片提取特征，是计算机视觉中常用的做法。DETR使用CNN来提取图片的多尺度特征图（如经典的ResNet）,抽取出的特征不但减少了模型的计算量，还加速模型训练时的收敛速度，并且在不同下游任务中拥有优异的泛化性。

*Transformer编码器-解码器架构*。特征提取骨干网络抽取的特征展平（flatten）后加上位置编码即为Query序列。Query序列经过多层编码器编码后传递给解码器，解码器与编码后的序列做交叉注意力后输出结果序列。结果序列经过简单的前馈神经网络后得到分类的结果。这里值得一提的是编码器的初始化输入序列是可以学习的参数，每一个Query代表着一个物体，称为物体序列（Object Query)，经过交叉注意力对特定物体进行聚合得到物体的精细位置。

*集合预测损失函数*。DETR将目标检测看作集合预测问题，其优化目标即输出集合与真实集合一致。这一优化目标需要模型预测全局的目标整体。为了将集合中的元素一一对应，真实集合中会加入空集元素代表无目标（DETR的预测元素个数固定且远大于真实目标个数），然后使用二分图匹配算法（匈牙利算法）来计算两个集合之间的最佳匹配，该匹配下的定位损失和分类损失作为评判的标准，即为损失函数。


=== 多视角端到端3D目标检测模型 DETR3D

DETR3D将DETR中基于Transformer的2D检测框架引入到了3D检测任务中：一次性生成N个bbox，采用set-to-set损失函数计算预测和GT的二分图匹配损失。这种方式避免了常规3D检测任务中所需的深度估计模块，因此无需集中算力进行冗余信息的处理，而只关注在目标的特征之上，速度得到了较大的提升，也避免了重建带来的误差。此外，DETR3D也无需NMS等后处理操作。

整个网络可大致分为三个部分：

*特征提取骨干网络*。
输入车载环视的6张图片,每张图片通过ResNet等2D骨干网络提取特征；再通过FPN得到4个不同尺度特征图

#figure(
  image("/images/Chapter2/DETR3D.png",height: 3cm),
  caption: [DETR3D模型示意图]
)<detr3d_fig>

*解码器*。
解码器的输入是特征提取骨干网络输出的特征图，在特征层面实现2D到3D的转换，避免深度估计带来的误差。
其初始化物体序列的生成类似DETR，随机生成$M$个Query。

接着如@detr3d_fig 中蓝线所示，使用子网络预测query在三维空间中的一个参考点（通常是简单的线性变换）。然后如绿线所示，利用相机内外参，将这个参考点反投影回图像中，找到其在原始图像中对应的位置。找到原始位置后将投影后的点对应到FPN中的每一个尺度的特征图上。

由于投影点经过下采样后在不同尺度的特征图上很可能没有刚好对应的特征点，因此采用双线性插值的方法来获取得到在每个尺度上的特征。将不同尺度上和不同位置相机上的提取到的特征进行求和平均处理，利用多头注意力机制，将找出的特征映射部分对物体序列进行修正。这种修正过程是逐层进行的，理论上，更靠后的层应该会吸纳更多的特征信息。

*集合预测损失函数*。这一部分与DETR相似，解码器的每一层输出都计算loss。回归损失采用L1损失，分类损失使用focal loss。




== 算法性能评价指标与方法

本项目的任务为3D目标检测，广泛运用的评价指标为NuScenes 3D指标，这是一种用于评估自动驾驶场景中的3D目标检测方法的综合指标。主要有mAP、NDS、mATE、mASE、mAOE、mAVE和mAAE七个指标。

#text(weight: "bold")[AP(Average Precision)]。  AP值的获取涉及PR曲线，PR曲线计算的是在某一分类阈值下查准率与召回率的曲线与坐标轴形成图形的面积。其中查准率与召回率的计算涉及混淆矩阵，TP（真正类）、FP（假正类）、FN（假负类）、TN（真负类），其中计算公式如下：
$
  "Precosion" = "TP" / ("TP" + "FP") \
  "Recall" = "TP" / ("TP" + "FN")
$
PR曲线的绘制是按照置信度排序后取前$n$个目标计算出的$P$（查准率）与$R$（召回率）坐标，当$n$从$1$取到$N$后便形成一条曲线。PR曲线较为客观的反应了分类方法的性能，而不同情况下算法的分类阈值不同，将不同阈值下的PR曲线面积求平均便得到了AP指数。在NuScenes中，由于目标分类不同，将所有类别的AP求平均即为mAP指数。

ATE（Average Translation Error），平均平移误差，即预测3D坐标中心点与真实框的中心点之间2D欧氏距离；ASE（Average Scale Error），平均尺度误差，即预测3D框的长宽高与真实框的差距，使用3DIoU指标；AOE（Average Orientation Error），平均方向误差，即预测3D框与真实框的偏航角差距； AVE （Average Velocity Error），平均速度误差，即预测的速度3维向量与真实值的差的L2范数；AAE（Average Attribute Error），平均属性误差，定义为1-准确率。

NDS指标为综合指标，为以上6个指标的加权平均值，公式如下，mTP代表上面五个指标的集合：
$
 "NDS" = 1/10 [5 "mAP" + Sigma_("mTP" in "TP")(1 - min(1, "mTP"))] 
$


== 本章小结

本章阐释了本文用到的相关理论和技术。本章首先说明了3D目标检测的数据集的选择和UAV3D数据集的格式。之后详细介绍了Transformer模型，从注意力机制开始深入讲解了Transformer的各个结构以及其作用。在此基础上，本章还引入了端到端的3D目标检测模型DETR3D，并介绍了3D目标检测任务中广泛使用的评价指标NuScenes 3D指标。



#{
  pagebreak()
}

= 多视角的时空特征结合端到端3D目标检测算法

== 引言

针对无人机的3D目标检测，上一章完成了面向无人机的3D目标检测数据集UAV3D、3D目标检测任务的详细介绍、Transformer以及DETR3D模型的介绍。本章将详细分析DETR3D和其他3D目标检测算法应用于UAV3D数据集的实验结果，主要基于DETR3D算法的实验结果提出进一步的改进方案。根据DETR3D的实验结果，提出融合数据集的时序信息，加入前一帧模型的预测信息，加速模型收敛，并且增强模型检测的鲁棒性。其次利用数据集中无人机的位姿信息，对比前后帧的位姿可以得到上一帧目标的到下一帧的位置变换矩阵，将从上一帧获取的目标Query经过旋转变换后得到当前帧的相对3D位置。最后在UAV3D数据集上对方法进行了验证。


== 现有算法在UAV3D数据集的检测效果

为了测试现有面向车端的多视角3D目标检测模型在UAV3D数据集上的表现，本文选取了BEVFusion、DETR3D和PETR三个经典的多视角3D目标检测模型，分别使用UAV3D数据集进行训练。

训练使用的环境配置为Python 3.8.20，使用RTX3090\*4 和V100\*4 GPU以及Intel(R) Xeon(R) Gold 6226\@2.90GHz 16核\*2。训练数据集UAV3D中的1000个场景（Scene）中70%作为训练集，15%作为验证集，剩下的15%作为测试集，进行训练。

实验结果如下表：

#{
  figure(
    table(
      columns: (auto,auto,auto,auto,auto,auto,auto,auto),
      // rows: auto,
      align: center,
      table.header([模型],[特征提取骨干网络],[图片大小],[*mAP$arrow.t$*],[*NDS$arrow.t$*],[*mATE$arrow.b$*],[*mASE$arrow.b$*],[*mAOE$arrow.b$*]),
      [BEVFusion],  [Res-101],[$800 times 450$],[0.536],[0.582],[0.521],[0.*154*],[0.343],
      [PETR],       [Res-50], [$800 times 450$],[0.581],[0.632],[0.625],[0.160],[*0.064*],
      [DETR3D],     [Res-101],[$800 times 450$],[*0.610*],[*0.671*],[*0.494*],[0.158],[0.070],
      
    ),
    caption: [BEVFusion、PETR、DETR3D模型在UAV3D数据集上的表现]
  )
}<prev_model_fig>

=== 实验结果分析

如@prev_model_fig 中数据所示，综合表现最佳的模型为DETR3D，虽然其mASE略低于BEVFusion而mAOE低于PETR，但都在同一水平，相差不大。而DETR3D的mAP和NDS指标明显优于PETR与BEVFusion模型。

PETR与BEVFusion模型在@1.2.5 中有关于总体的介绍。其中BEVFusion模型支持多模态输入，在NuScenes数据集上的相机与激光雷达多模态输入的mAP指标为68.52%，而多视角相机的得分仅为35.56%。UAV3D数据集面向多视角的相机，所以BEVFusion的表现差于PETR和DETR3D。PETR模型综合表现上略差于DETR3D模型，主要原因在于PETR使用的特征提取骨干网络与DETR3D不同，提取的图像特征不够精细与鲁棒，影响后续结构的训练。

接下来我们将详细分析DETR3D模型的预测结果，并提出改进的思路。

#{
  figure(
    grid(
    columns: 2,
    gutter: 3pt,  // 图片间距
    image("images/Chapter3/RIGHT_gt.png", width: 100%),
    image("images/Chapter3/RIGHT_pred.png", width: 100%),
    ),
  caption: [真实值（左）与DETR3D预测值（右）的对比]
  )
} <detr3d_pred_gt_fig>

@detr3d_pred_gt_fig 是选取的一张右侧摄像头的图片，左边是真实值，右边是DETR3D预测的3D边界框。由于DETR3D使用物体序列来作为物体的位置，其默认个数达到900个，所以右侧图会有较多的3D边界框，可以代表当前图片中DETR3D模型关注的区域。

对比左右图，可以发现DETR3D目前存在的缺点如下：

1、*部分遮挡物体检测失败。* 可以看到，对于部分遮挡的车辆，DETR3D并没有较好的识别，这说明DETR3D模型的鲁棒性不足。


2、*非目标物体分类错误。* DETR3D的Query目标集中在真实值的附近，有时候也会在无车的道路上识别出车辆，尽管模型给出该边界框的置信度较低，但也说明模型没能很好的区分目标与非目标物体。该问题的根源在于第三点，由于数据集中目标时常出现在建筑之后，这一部分目标的图像特征往往呈现为建筑的图像特征，导致模型难以区分。


3、*遮挡物体预测错误。* 第三点的问题根源与第二点相同，数据集中被建筑物或者树木等环境遮挡的目标车辆难以准确预测。完全遮挡物体的预测本身是一个非常有挑战性的任务，这需要模型对于目标的运动轨迹拥有良好的建模能力，并且模拟物体与环境的交互方式以推测遮挡目标的位置信息。


#{
  figure(
    grid(
    columns: 4,
    rows:2,
    gutter: 3pt,  // 图片间距
    image("images/Chapter3/cropped_gt_1.png", width: 4cm , height: 4cm,),
    image("images/Chapter3/cropped_pred_1.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_2.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_2.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_3.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_3.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_gt_4.png", width: 4cm, height: 4cm,),
    image("images/Chapter3/cropped_pred_4.png", width: 4cm, height: 4cm,),
    ),
  caption: [局部放大对比]
  )
}

==  时空特征融合的3D目标检测算法

基于DETR3D模型的实验结果，本文提出融合时序特征以提升模型的鲁棒性和对于遮挡物体的检测性能。对于遮挡物体，由于无人机难以捕捉其空间信息，可以融合时间信息，从先前帧中获取目标的历史位置。基于历史位置，模型结合无人机本体的移动轨迹和目标的移动速度可以更好的预测遮挡物体的3D位置信息。

考虑到Transformer的自回归特性，本文从Query序列着手。在DETR3D模型，Query序列中一个Token长度为256，经过一层神经网络后得到长度为3的张量，代表物体的3D坐标$x,y,z$。而Transformer中具有6层的解码器结构，Query序列每次经过一次解码操作都会发生变化，这是解码器根据Query的坐标对图像采样后进行的位置修正。所以在最后一层解码器得到的输出Query即为最终的预测结果。

为了融合历史帧的时序信息，可以将上一帧Transformer最后输出的Query序列保留，依照模型给出的置信度分数从高到低截取若干个Query。如此便获取到目标在上一帧的位置，将其作为当前帧的Transformer的Query序列的一部分。而Tranformer的每一层解码器都会将Query经过交叉注意力机制即进行修正。



== 实验结果


== 本章小结